<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-08-02T16:16:52+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Vignesh Ragupathy</title><subtitle>personal description</subtitle><author><name>Vignesh Ragupathy</name><email>r.vignesh88@gmail.com</email></author><entry><title type="html">Kubernetes monitoring in Zabbix via Prometheus backend</title><link href="http://0.0.0.0:4000/kubernetes-monitoring-in-zabbix-via-prometheus-backend/" rel="alternate" type="text/html" title="Kubernetes monitoring in Zabbix via Prometheus backend" /><published>2022-07-01T10:00:00+00:00</published><updated>2022-07-01T10:00:00+00:00</updated><id>http://0.0.0.0:4000/kubernetes-monitoring-in-zabbix-via-prometheus-backend</id><content type="html" xml:base="http://0.0.0.0:4000/kubernetes-monitoring-in-zabbix-via-prometheus-backend/"><![CDATA[<h2 id="summary">Summary</h2>

<p>Monitoring in Kubernetes is a complex task.</p>

<p>The traditional monitoring framework is not sufficient to handle such a massive workload.</p>

<p>Zabbix since version 6.0 provides a native way of integration for monitoring Kubernetes cluster.</p>

<p>Zabbix-Kubernetes integration provides various templates to monitor kubernetes components like <code class="language-plaintext highlighter-rouge">kube-controller-manager</code>, <code class="language-plaintext highlighter-rouge">kube-apiserver</code>, <code class="language-plaintext highlighter-rouge">kube-scheduler</code>, <code class="language-plaintext highlighter-rouge">kubelet</code>, etc.</p>

<p>It also supports automatic discovery of kubernetes nodes, pods and also collects metrics agentlessly.</p>

<h2 id="why-i-dont-like-the-zabbixs-direct-way-of-monitoring-kubernetes-cluster">Why I don’t like the Zabbix’s direct way of monitoring Kubernetes cluster?</h2>

<p>Although Zabbix-Kubernetes integration looks promising in the beginning , it is not easy to use.</p>

<p>The documentation is not clear, it is mostly based on the assumption that the Zabbix server is running inside the same Kubernetes cluster.</p>

<p>There are so much details missing in the documentation especially if you want to do the monitoring from external Zabbix server or even to do a multiple cross cluster monitoring.</p>

<p>And finally, most of all, the  Zabbix monitoring system is not designed to monitor <code class="language-plaintext highlighter-rouge">directly</code> such a volatile, dynamic, multi-dimensional metrics from infrastructure like Kubernetes.</p>

<h2 id="why-prometheus">Why Prometheus?</h2>

<p>Prometheus is an opensource monitoring system which can collect massive amount of data in near real-time using it’s pull-based mechanism.</p>

<p>Prometheus can be easily configured for service discovery.</p>

<p>It is agentless and works by sending an HTTP request so-called scrape, which can find the endpoints and scrap metrics from any source.</p>

<p>The response from scrape request is parsed and stored in Prometheus database.</p>

<p>Prometheus provided powerful querying using it’s PromQL and the metrics collected can be pulled from any external system like Zabbix or Grafana via its API.</p>

<blockquote>
  <p>Using Prometheus federation, we can easily collect metrics from multiple Kubernetes clusters and create a hierarchically scaled monitoring system.</p>
</blockquote>

<h2 id="how-prometheus-exposes-metrics">How Prometheus exposes metrics?</h2>

<p>Prometheus collects the metrics from any source by scraping the HTTP endpoint which contains metrics.</p>

<p>It also provides a web interface to view the metrics.</p>

<p>The metrics are exposed via API.</p>

<p>Prometheus metrics can be queried by any HTTP client using it’s API Endpoints.</p>

<p><strong>Query Prometheus using curl</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -X GET  'https://PROMETHEUS_HOSTNAME/api/v1/query?query=up' | jq status.
</code></pre></div></div>

<p><strong>Query on kubernetes node metrics</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -X GET  'https://PROMETHEUS_HOSTNAME/api/v1/query?query=kube_node_info' | jq status.
</code></pre></div></div>

<p><strong>Advanced query operations like boolean</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -X GET  'http://PROMETHEUS_HOSTNAME/api/v1/query?query={__name__=~"kube_pod_container_resource_limits_cpu_cores|kube_pod_status_phase"}&gt;0' | jq .
</code></pre></div></div>

<h2 id="why-prometheus-is-just-not-good-enough">Why Prometheus is just not good enough?</h2>

<p>Prometheus is good for collecting the metrics from Kubernetes cluster.
But it is not enough to act as a full-stack monitoring system.</p>

<p>Here are some of the limitations.</p>

<ul>
  <li>Very basic visualization of the metrics.</li>
  <li>No AI/ML features like automatic anomaly detection</li>
  <li>No role based access control</li>
  <li>Long term storage is not available.</li>
</ul>

<h2 id="proposed-approach">Proposed approach</h2>

<blockquote>
  <p>The proposed approach is to use Prometheus as a backend for Zabbix.</p>
</blockquote>

<p>This approach provides the flexibility of using Prometheus pull based metrics collection, scalability and PromQL queries. Also the advantage of Zabbix for alerting, anomaly detection and role based access control etc.</p>

<p><strong>Architecture of the proposed approach</strong></p>

<p><img src="/content/images/2022/kubernetes-promethues-federate.png" alt="prometheus_federate" /></p>]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="opensource" /><category term="observability" /><summary type="html"><![CDATA[Summary]]></summary></entry><entry><title type="html">My life at SBI</title><link href="http://0.0.0.0:4000/lifeatsbi/" rel="alternate" type="text/html" title="My life at SBI" /><published>2021-09-16T12:15:00+00:00</published><updated>2021-09-16T12:15:00+00:00</updated><id>http://0.0.0.0:4000/lifeatsbi</id><content type="html" xml:base="http://0.0.0.0:4000/lifeatsbi/"><![CDATA[<h2 id="summary">Summary</h2>

<p>I joined State Bank of India on July 2020 and worked for 1 year and 2 months as Manager - IT Infrastructure Architect in Enterprise and Technology Architecture department in GITC, Navi Mumbai.</p>

<p>My role in SBI is technical, and it involves consulting, design, Architecture and reviewing of application and infrastructure.</p>

<p>Through out my tenure in SBI i worked on various short term projects. Among them setting up DevOps infrastructure for SBI is one.</p>

<p>It’s an ongoing project and we’re constantly finding ways to improve and make life easier for developers and Operations team across SBI.</p>

<p>Breakup of overall time that had spent in SBI.</p>

<p><img src="/content/images/2021/sbi_work_breakup.png" alt="sbi_breadkdown" /></p>

<h2 id="what-have-i-learned-">What have I learned ?</h2>

<ul>
  <li>Importance of architecture principles and policies</li>
  <li>Some software architectural design</li>
  <li>Compliance and regulation in banking</li>
  <li>DevOps tools and practises</li>
  <li>Managing vendor partners</li>
  <li>Tracking project timelines</li>
</ul>

<h4 id="certificate-of-appreciation-for-delivering-a-talk-on-kubernetes">Certificate of appreciation for delivering a talk on Kubernetes</h4>

<iframe src="https://www.linkedin.com/embed/feed/update/urn:li:ugcPost:6757560991862669312" height="448" width="504" frameborder="0" allowfullscreen="" title="Embedded post"></iframe>

<h2 id="my-timeline-in-sbi">My timeline in SBI</h2>

<ul>
  <li>Sep 2019 - Specialist exam for SBI</li>
  <li>Dec 2019 - Personal Interview in Mumbai, SBI</li>
  <li>Jan 2020 -  Received job offer from SBI</li>
  <li>Feb 2020 - Pandemic and full work from home start in Ericsson</li>
  <li>Apr 2021 - Relieving extension in Ericsson</li>
  <li>Jun 2020 - Last day in Ericsson</li>
  <li>Jul 2020 - Relocation to Mumbai and joining SBI</li>
  <li>Aug 2020 - Joining Enterprise and Technology architecture dept in SBI</li>
  <li>Oct 2020 - Travel to native and bringing family to Mumbai</li>
  <li>April 2021 - Travel to native and leaving family</li>
  <li>May 2021 - COVID like symptoms and home quarantine</li>
  <li>June 2021 - Decision to leave SBI and relocate from Mumbai</li>
  <li>July 2021 - New job offer received</li>
  <li>Aug 2021 - Resignation initiation in SBI</li>
  <li>Sep 2021 - Last working day in SBI</li>
</ul>

<h2 id="why-leaving-job-from-state-bank-of-india">Why leaving job from State Bank of India</h2>

<p>The decision for leaving the job at state bank of India is both personal and family. Some of the key points that influence the decisions are no WFH, no relocation/transfer of job, micro management, unnecessary and complicated process, poor decision making and low learning curve/Legacy technologies.</p>

<p>One of the important factor that any organisation should focus is equally changing the organisation culture when they modernise the technology stack. There are significant effort put on hiring technical experts, digital transformation, cloud strategy but very little on organisation culture.</p>

<h2 id="to-sbi-on-request-from-sbi-colleagues">To SBI (on request from SBI colleagues)</h2>

<ul>
  <li>Invest as equally as technology transformation in cultural transformation.</li>
  <li>Let the internal team solve the hard problems and use the external teams /vendors only to enable the in-house teams</li>
  <li>IT operations need engineers, not just the people managers. Replace people managers with engineering managers whereever needed. Do not mix them both.</li>
  <li>Make an open/transparent and employee friendly organisation.</li>
</ul>

<h2 id="so-whats-next-">So whats next ?</h2>

<p>So what am I doing next? It’s been a privilege and an adventure to work in SBI as Infrastructure Architect with so many amazing people. But I’m now excited about my new role as Sr Site Reliability Engineer at Sage Intacct, Bangalore.</p>]]></content><author><name>Vignesh Ragupathy</name></author><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[Summary]]></summary></entry><entry><title type="html">Plotly4Nagios - A Graph plugin for nagios monitoring</title><link href="http://0.0.0.0:4000/plotly4nagios-graph-plugin-for-nagios-monitoring/" rel="alternate" type="text/html" title="Plotly4Nagios - A Graph plugin for nagios monitoring" /><published>2021-03-24T12:15:00+00:00</published><updated>2021-03-24T12:15:00+00:00</updated><id>http://0.0.0.0:4000/plotly4nagios-graph-plugin-for-nagios-monitoring</id><content type="html" xml:base="http://0.0.0.0:4000/plotly4nagios-graph-plugin-for-nagios-monitoring/"><![CDATA[<p><a href="https://github.com/vigneshragupathy/plotly4nagios">Plotly4Nagios</a> is a nagios plugin to display the performance data in Graph. It uses the RRD database provided by pnp4nagios and visualize it in interactive graph format using plotly javascript. The first pre-release is published today in <a href="https://github.com/vigneshragupathy/plotly4nagios">github</a> and here is the installation document. You can experiment it and report the issue/feedback for further enhancement.</p>

<blockquote>
  <p>Plotly4Nagios is accepted and listed under official <a href="https://exchange.nagios.org/directory/Addons/Graphing-and-Trending/Plotly4Nagios/details">nagios addons</a></p>
</blockquote>

<h2 id="git-badges">GIT badges</h2>

<p><img src="https://img.shields.io/github/license/vigneshragupathy/plotly4nagios" alt="GitHub" />
<a href="https://travis-ci.com/vigneshragupathy/plotly4nagios"><img src="https://travis-ci.com/vigneshragupathy/plotly4nagios.svg?branch=main" alt="Build Status" /></a></p>

<h2 id="features">Features</h2>

<ul>
  <li>Easy integration with nagios <code class="language-plaintext highlighter-rouge">notes_url</code>.</li>
  <li>Single page view for all performance metrics.</li>
  <li>Easy template change using configuration variable.</li>
  <li>Docker container based deploy and run.</li>
</ul>

<h2 id="prerequisite">Prerequisite</h2>

<ul>
  <li><a href="https://support.nagios.com/kb/article/nagios-core-performance-graphs-using-pnp4nagios-801.html">pnp4nagios</a></li>
</ul>

<h2 id="installation">Installation</h2>

<ul>
  <li>Download plotly4nagios.tar.gz and extract it under /usr/local/plotly4nagios</li>
  <li>Modify the config.json variables according to the environment</li>
  <li>Copy the plotly4nagios/plotly4nagios.conf to /etc/http/conf.d/ folder and restart httpd</li>
  <li>Add the follwing with  notes_url to templates.cfg.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    notes_url /plotly4nagios/plotly4nagios.html?host<span class="o">=</span><span class="se">\$</span>HOSTNAME<span class="se">\$</span>&amp;srv<span class="o">=</span>_HOST_
    notes_url /plotly4nagios/plotly4nagios.html?host<span class="o">=</span><span class="se">\$</span>HOSTNAME<span class="nv">$&amp;srv</span><span class="o">=</span><span class="se">\$</span>SERVICEDESC<span class="err">$</span>
</code></pre></div></div>

<ul>
  <li>Restart httpd and nagios.</li>
</ul>

<h2 id="installation-with-dockerubuntu-image">Installation with docker(Ubuntu image)</h2>

<ul>
  <li>Build the docker image using the below command</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/vigneshragupathy/plotly4nagios.git
<span class="nb">cd </span>plotly4nagios
docker build <span class="nt">-t</span> plotly4nagios <span class="nb">.</span>
</code></pre></div></div>

<ul>
  <li>Run the docker container using the below command</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-it</span> <span class="nt">--name</span> plotly4nagios <span class="nt">-p</span> 80:80 plotly4nagios
</code></pre></div></div>

<p>Alternatively direct pull and run from docker hub.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-d</span> <span class="nt">-p</span> 80:80 <span class="nt">--name</span> plotly4nagios vigneshragupathy/plotly4nagios
</code></pre></div></div>

<blockquote>
  <p>Open from the browser and view the application at http://localhost/nagios</p>
</blockquote>

<h3 id="login-details">Login details</h3>

<ul>
  <li>Username : nagiosadmin</li>
  <li>Password : nagios</li>
</ul>

<h2 id="demo">Demo</h2>

<p><img src="https://raw.githubusercontent.com/vigneshragupathy/plotly4nagios/main/img/plotly4nagios.gif" alt="'demo'" /></p>

<h2 id="screenshot">Screenshot</h2>

<h3 id="dark-mode">Dark mode</h3>

<p><img src="https://raw.githubusercontent.com/vigneshragupathy/plotly4nagios/main/img/screenshot_darkmode.png" alt="'Dark mode'" /></p>

<h2 id="license">License</h2>

<p>Copyright 2020-2021 © Vignesh Ragupathy. All rights reserved.</p>

<p>Licensed under the <a href="https://github.com/vigneshragupathy/plotly4nagios/blob/ed09f8d687014107c8002d92acbc7acd2f62468a/LICENSE">MIT License</a></p>]]></content><author><name>Vignesh Ragupathy</name></author><category term="linux" /><category term="opensource" /><category term="observability" /><summary type="html"><![CDATA[Plotly4Nagios is a nagios plugin to display the performance data in Graph. It uses the RRD database provided by pnp4nagios and visualize it in interactive graph format using plotly javascript. The first pre-release is published today in github and here is the installation document. You can experiment it and report the issue/feedback for further enhancement.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/content/images/2021/plotly4nagios_dark.png" /><media:content medium="image" url="http://0.0.0.0:4000/content/images/2021/plotly4nagios_dark.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Publish package in NPM and serve the static content from CDN</title><link href="http://0.0.0.0:4000/publish-package-in-npm-and-serve-the-static-content-from-cdn/" rel="alternate" type="text/html" title="Publish package in NPM and serve the static content from CDN" /><published>2020-06-12T12:15:00+00:00</published><updated>2020-06-12T12:15:00+00:00</updated><id>http://0.0.0.0:4000/publish-package-in-npm-and-serve-the-static-content-from-cdn</id><content type="html" xml:base="http://0.0.0.0:4000/publish-package-in-npm-and-serve-the-static-content-from-cdn/"><![CDATA[<p><img src="/content/images/cover/npm.jpg" alt="" />
<em>Photo by <a href="https://photography.vikki.in/vikki-photography-budapest-3" target="_blank">Vignesh Ragupathy</a>.</em></p>

<p>I have been utilizing AWS to host my personal blog for almost 3 years now. Originally my blog was hosted in WordPress and then I migrated to <a href="https://ghost.org/" target="_blank">ghost</a>. It’s been 2 years now in ghost and I thought of exploring a new hosting option which should be free, supports custom domain name and free <a href="https://letsencrypt.org/" target="_blank">SSL</a>.</p>

<p><a href="https://jekyllrb.com/" target="_blank">Jekyll</a> is a ruby based static blog generator and it has an advantage of free hosting in GitHub. The letsencrypt SSL certificate is also provided by GitHub for my custom domain so i don’t have to worry about managing it.</p>

<p>I also created a separate <a href="https://tools.vikki.in" target="_blank">website</a> to showcase my open-source tools and I can use the same AWS instance for hosting it. It is a Django application which uses more memory/CPU, so i can run it in a dedicated instance instead of running the ghost and Django together.</p>

<p>One of the challenges in a Django application is hosting your static content. Django recommends using a proxy server like Nginx to serve its static content.</p>

<p>I use my nginx proxy to serve the static content. But due to performance reason , i started to explore the free CDN to serve my static content</p>

<p>Below is the nginx configuration snippet for mapping static content.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="go">location /static/ {
</span><span class="gp">        root /tools.vikki.in/static;</span><span class="w">
</span><span class="go">    }</span></code></pre></figure>

<p>After doing some research I chose to utilize unpkg or jsdelivr for my site.</p>

<blockquote>
  <p>unpkg and jsdelivr are global CDN and they can be used to deliver any packages hosted in NPM</p>
</blockquote>

<p><a href="https://unpkg.com/" target="_blank">unpkg</a> and <a href="https://www.jsdelivr.com/" target="_blank">jsdelivr</a> both provides CDN for the content hosted in NPM.
So first we should have the static content published in <a href="https://www.npmjs.com/" target="_blank">NPM</a>.</p>

<h2 id="npm-package-creation">NPM Package creation</h2>

<h3 id="1-create-the-directory-for-adding-packages-for-npm">1. Create the directory for adding packages for NPM</h3>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="go">mkdir npm
mkdir npm/dist
cd npm</span></code></pre></figure>

<h3 id="2-create-a-packagejson-file-for-your-package">2. Create a package.json file for your package</h3>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="go">npm init</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="go">This utility will walk you through creating a package.json file.
It only covers the most common items, and tries to guess sensible defaults.

See `npm help json` for definitive documentation on these fields
and exactly what they do.

Use `npm install pkg` afterwards to install a package and
save it as a dependency in the package.json file.

Press ^C at any time to quit.
package name: (npm) vikki-tools
version: (1.0.0) 1.0.7
description: Libraries for https://tools.vikki.in
entry point: (index.js) dist/index.js
test command:
git repository: https://github.com/vignesh88/tools.git
keywords: vikki, tools
author: Vignesh Ragupathy
license: (ISC)
About to write to /home/vikki/npm/package.json:</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="code"><pre><span class="p">{</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"vikki-tools"</span><span class="p">,</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="nl">"version"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1.0.7"</span><span class="p">,</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Libraries for https://tools.vikki.in"</span><span class="p">,</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="nl">"main"</span><span class="p">:</span><span class="w"> </span><span class="s2">"dist/index.js"</span><span class="p">,</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="nl">"scripts"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="err"> </span><span class="w"> </span><span class="nl">"test"</span><span class="p">:</span><span class="w"> </span><span class="s2">"echo </span><span class="se">\"</span><span class="s2">Error: no test specified</span><span class="se">\"</span><span class="s2"> &amp;&amp; exit 1"</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="p">},</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="nl">"repository"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="err"> </span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"git"</span><span class="p">,</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="err"> </span><span class="w"> </span><span class="nl">"url"</span><span class="p">:</span><span class="w"> </span><span class="s2">"git+https://github.com/vignesh88/tools.git"</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="p">},</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="nl">"keywords"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="err"> </span><span class="w"> </span><span class="s2">"vikki"</span><span class="p">,</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="err"> </span><span class="w"> </span><span class="s2">"tools"</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="p">],</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="nl">"author"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Vignesh Ragupathy"</span><span class="p">,</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="nl">"license"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ISC"</span><span class="p">,</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="nl">"bugs"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="err"> </span><span class="w"> </span><span class="nl">"url"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://github.com/vignesh88/tools/issues"</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="p">},</span><span class="w">
</span><span class="err"> </span><span class="w"> </span><span class="nl">"homepage"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://github.com/vignesh88/tools#readme"</span><span class="w">
</span><span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="go">Is this OK? (yes) yes</span></code></pre></figure>

<h3 id="3-create-a-indexjs">3. Create a index.js</h3>

<p>I added a javascript function that will be used to copy text to clipboard.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="go">vim dist/index.js</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre><span class="kd">function</span> <span class="nx">copyToClipboard</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="nx">y</span><span class="p">)</span> <span class="p">{</span>
<span class="err"> </span> <span class="err"> </span> <span class="k">if</span><span class="p">(</span> <span class="nb">document</span><span class="p">.</span><span class="nx">getElementById</span><span class="p">(</span><span class="nx">x</span><span class="p">).</span><span class="nx">value</span><span class="p">)</span> <span class="p">{</span>
<span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="nx">data_2_copy</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">getElementById</span><span class="p">(</span><span class="nx">x</span><span class="p">).</span><span class="nx">value</span><span class="p">;</span>
<span class="err"> </span> <span class="err"> </span> <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
<span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="nx">data_2_copy</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">getElementById</span><span class="p">(</span><span class="nx">x</span><span class="p">).</span><span class="nx">innerText</span><span class="p">;</span>
<span class="err"> </span> <span class="err"> </span> <span class="p">}</span>

<span class="err"> </span> <span class="err"> </span> <span class="kd">var</span> <span class="nx">e</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">createElement</span><span class="p">(</span><span class="dl">"</span><span class="s2">textarea</span><span class="dl">"</span><span class="p">);</span>
<span class="err"> </span> <span class="err"> </span> <span class="nx">e</span><span class="p">.</span><span class="nx">style</span><span class="p">.</span><span class="nx">opacity</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">0</span><span class="dl">"</span><span class="p">,</span> <span class="nx">e</span><span class="p">.</span><span class="nx">style</span><span class="p">.</span><span class="nx">position</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">fixed</span><span class="dl">"</span><span class="p">,</span> <span class="nx">e</span><span class="p">.</span><span class="nx">textContent</span> <span class="o">=</span> <span class="nx">data_2_copy</span><span class="p">;</span>
<span class="err"> </span> <span class="err"> </span> <span class="kd">var</span> <span class="nx">t</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">getElementsByTagName</span><span class="p">(</span><span class="dl">"</span><span class="s2">body</span><span class="dl">"</span><span class="p">)[</span><span class="mi">0</span><span class="p">];</span>
<span class="err"> </span> <span class="err"> </span> <span class="nx">t</span><span class="p">.</span><span class="nx">appendChild</span><span class="p">(</span><span class="nx">e</span><span class="p">),</span> <span class="nx">e</span><span class="p">.</span><span class="nx">select</span><span class="p">(),</span> <span class="nb">document</span><span class="p">.</span><span class="nx">execCommand</span><span class="p">(</span><span class="dl">"</span><span class="s2">copy</span><span class="dl">"</span><span class="p">),</span> <span class="nx">t</span><span class="p">.</span><span class="nx">removeChild</span><span class="p">(</span><span class="nx">e</span><span class="p">),</span> <span class="nx">$</span><span class="p">(</span><span class="nx">y</span><span class="p">).</span><span class="nx">show</span><span class="p">(),</span> <span class="nx">setTimeout</span><span class="p">((</span><span class="kd">function</span> <span class="p">()</span> <span class="p">{</span>
<span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="nx">$</span><span class="p">(</span><span class="nx">y</span><span class="p">).</span><span class="nx">hide</span><span class="p">()</span>
<span class="err"> </span> <span class="err"> </span> <span class="p">}),</span> <span class="mi">1</span><span class="nx">e3</span><span class="p">)</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="4-copy-all-your-static-content-to-dist-directory">4. Copy all your static content to dist directory</h3>

<p>Now lets copy all our static content to the <mark>dist</mark> directory.
I have various css,images,javascript that will be used in various app inside my django application.</p>

<p>Below are the files which i copied.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="go">tree .
</span><span class="c">.
</span><span class="go">├── dist
│   ├── admin
│   │   ├── css
│   │   │   ├── autocomplete.css
│   │   │   ├── base.css
│   ├── geoip
│   │   ├── css
│   │   │   ├── geoip_dark.css
│   │   │   └── geoip_light.css
│   │   └── js
│   │       └── geoip.js
│   ├── index.js
│   └── password_generator
│       ├── css
│       │   ├── password_generator_dark.css
│       │   └── password_generator_light.css
│       ├── img
│       │   ├── copy-full.svg
│       │   └── regenerate.svg
│       └── js
│           └── password_generator.js
├── package.json
└── README.md</span></code></pre></figure>

<h3 id="5-publish-you-static-content-as-package-in-npm">5. Publish you static content as package in NPM</h3>

<p>Now we are all set, let’s connect to NPM and publish our package.</p>

<blockquote>
  <p>You should already have an account in NPM to publish.</p>
</blockquote>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="go">npm login
Username: r_vignesh
Password: 
Email: (this IS public) me@vikki.in
Logged in as r_vignesh on https://registry.npmjs.org/.</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="go">npm publish

npm notice
npm notice package: vikki-tools@1.0.7
npm notice === Tarball Contents ===
npm notice 1.1kB   dist/admin/img/LICENSE
npm notice 8.4kB   dist/admin/css/autocomplete.css
npm notice 16.4kB  dist/admin/css/base.css
npm notice 698B    dist/base64/css/base64_dark.css
npm notice 159B    dist/base64/css/base64_light.css
npm notice 85.9kB  dist/admin/fonts/Roboto-Regular-webfont.woff
npm notice === Tarball Details ===
npm notice name:          vikki-tools
npm notice version:       1.0.7
npm notice package size:  1.1 MB
npm notice unpacked size: 3.9 MB
npm notice shasum:        a9153c3a9bb68bc34d5040d2088a5b95a256e4cc
npm notice integrity:     sha512-zynWl1/pL0Wvk[...]k3yhkCzBz7+0A==
npm notice total files:   188
npm notice
+ vikki-tools@1.0.7</span></code></pre></figure>

<p>That’s it. Now we have the package published in NPM.</p>

<blockquote>
  <p>Unpkg and Jsdelivr provide CDN for NPM packages without any configuration.</p>
</blockquote>

<h3 id="6-verify-published-package-in-npm">6. Verify published package in NPM</h3>

<p>Lets try to access it using unpkg. Open your browser and enter the url in the below format.</p>

<p><mark>https://unpkg.com/pacakage/</mark></p>

<p>For using specific version <mark>https://unpkg.com/package@version/:file</mark></p>

<p>My package name is <em>vikki-tools</em> so the format will be <a href="https://unpkg.com/vikki-tools/" target="_blank">https://unpkg.com/vikki-tools<mark>/</mark></a>.</p>

<blockquote>
  <p>The leading <mark> / </mark> at the end of the URL is important.</p>
</blockquote>

<h3 id="screenshot-from-browser">Screenshot from browser</h3>

<p><img src="/content/images/2020/screenshot_vikki_tools.jpg" alt="unpkg screenshot" /></p>

<h2 id="using-unpkg-to-serve-static-content-in-website">Using Unpkg to serve static content in website</h2>

<p>We can now load the static content from NPM on our website.</p>

<figure class="highlight"><pre><code class="language-html" data-lang="html"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://unpkg.com/vikki-tools@1.0.3/dist/index.js"</span><span class="nt">&gt;&lt;/script&gt;</span>
<span class="nt">&lt;link</span> <span class="na">href=</span><span class="s">"https://unpkg.com/vikki-tools@1.0.3/dist/base64/css/base64_dark.css"</span> <span class="na">rel=</span><span class="s">"stylesheet"</span><span class="nt">&gt;</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h2 id="using-jsdelivr-to-serve-static-content-in-website">Using Jsdelivr to serve static content in website</h2>

<p>We can also use Jsdelivr instead of unpkg.</p>

<figure class="highlight"><pre><code class="language-html" data-lang="html"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/vikki-tools@1.0.3/dist/index.js"</span><span class="nt">&gt;&lt;/script&gt;</span>
<span class="nt">&lt;link</span> <span class="na">href=</span><span class="s">"https://cdn.jsdelivr.net/npm/vikki-tools@1.0.3/dist/base64/css/base64_dark.css"</span> <span class="na">rel=</span><span class="s">"stylesheet"</span><span class="nt">&gt;</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h2 id="auto-minified-version-from-jsdelivr">Auto minified version from jsdelivr</h2>

<p>Jsdelivr also provides the auto minified version of the CSS and Javascript from NPM.
If you want to use minified version css and js, just add <mark>.min</mark> extension to the filename</p>

<figure class="highlight"><pre><code class="language-html" data-lang="html"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/vikki-tools@1.0.3/dist/index.min.js"</span><span class="nt">&gt;&lt;/script&gt;</span>
<span class="nt">&lt;link</span> <span class="na">href=</span><span class="s">"https://cdn.jsdelivr.net/npm/vikki-tools@1.0.3/dist/base64/css/base64_dark.min.css"</span> <span class="na">rel=</span><span class="s">"stylesheet"</span><span class="nt">&gt;</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h2 id="script-to-automatically-update-the-static-and-cdn-url-in-django">Script to automatically update the static and CDN URL in Django</h2>

<p>For ease, I created a script to automatically update all static content in your template directory in the Django application.</p>

<p>The code is available in the <a href="https://github.com/vignesh88/tools/blob/master/vikki_scripts/django_template_static_to_cdn.py" target="_blank">Github URL</a></p>

<h2 id="demo-video">Demo video</h2>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container"><iframe src="https://www.youtube.com/embed/PKGLkmzHQH0" frameborder="0" allowfullscreen=""></iframe></div>

<!--
<object style="width:100%;height:100%;width: 820px; height: 461.25px; float: none; clear: both; margin: 2px auto;" data="https://www.youtube.com/embed/XMSV5J4jxSo">
</object>
<iframe width="560" height="315" src="https://www.youtube.com/embed/XMSV5J4jxSo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
-->]]></content><author><name>Vignesh Ragupathy</name></author><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[Photo by Vignesh Ragupathy.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/content/images/cover/npm.jpg" /><media:content medium="image" url="http://0.0.0.0:4000/content/images/cover/npm.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Backup of etcd database in kubernetes</title><link href="http://0.0.0.0:4000/backup-of-etcd-database-in-kubernetes/" rel="alternate" type="text/html" title="Backup of etcd database in kubernetes" /><published>2019-11-28T16:37:26+00:00</published><updated>2019-11-28T16:37:26+00:00</updated><id>http://0.0.0.0:4000/backup-of-etcd-database-in-kubernetes</id><content type="html" xml:base="http://0.0.0.0:4000/backup-of-etcd-database-in-kubernetes/"><![CDATA[<p>Kubernetes cluster state is saved in etcd datastore. In the post we are going to see how to take a backup for etcd database in kubernetes cluster.</p>

<h3 id="setup"><strong>Setup</strong></h3>

<p>I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.</p>

<!--kg-card-begin: image-->
<figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-7.jpg" class="kg-image" /></figure>
<!--kg-card-end: image-->
<h5 id="step-1-create-a-directory-and-backup-all-certificates">Step 1: Create a directory and backup all certificates</h5>

<p>Kubernetes cluster have all the certificates saved in the defautl path /etc/kubernetes/pki. Take the backup of all the files and save it in the backup directory</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span><span class="nb">mkdir </span>backup 
<span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span><span class="nb">cd </span>backup/</code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/backup$</span><span class="w"> </span><span class="nb">sudo cp</span> <span class="nt">-rvf</span> /etc/kubernetes/pki <span class="nb">.</span>
<span class="gp">'/etc/kubernetes/pki' -&gt;</span><span class="w"> </span><span class="s1">'./pki'</span>
<span class="gp">'/etc/kubernetes/pki/ca.key' -&gt;</span><span class="w"> </span><span class="s1">'./pki/ca.key'</span>
<span class="gp">'/etc/kubernetes/pki/ca.crt' -&gt;</span><span class="w"> </span><span class="s1">'./pki/ca.crt'</span>
<span class="gp">'/etc/kubernetes/pki/apiserver.key' -&gt;</span><span class="w"> </span><span class="s1">'./pki/apiserver.key'</span>
<span class="gp">'/etc/kubernetes/pki/apiserver.crt' -&gt;</span><span class="w"> </span><span class="s1">'./pki/apiserver.crt'</span>
<span class="gp">'/etc/kubernetes/pki/apiserver-kubelet-client.key' -&gt;</span><span class="w"> </span><span class="s1">'./pki/apiserver-kubelet-client.key'</span>
<span class="gp">'/etc/kubernetes/pki/apiserver-kubelet-client.crt' -&gt;</span><span class="w"> </span><span class="s1">'./pki/apiserver-kubelet-client.crt'</span>
<span class="gp">'/etc/kubernetes/pki/front-proxy-ca.key' -&gt;</span><span class="w"> </span><span class="s1">'./pki/front-proxy-ca.key'</span>
<span class="gp">'/etc/kubernetes/pki/front-proxy-ca.crt' -&gt;</span><span class="w"> </span><span class="s1">'./pki/front-proxy-ca.crt'</span>
<span class="gp">'/etc/kubernetes/pki/front-proxy-client.key' -&gt;</span><span class="w"> </span><span class="s1">'./pki/front-proxy-client.key'</span>
<span class="gp">'/etc/kubernetes/pki/front-proxy-client.crt' -&gt;</span><span class="w"> </span><span class="s1">'./pki/front-proxy-client.crt'</span>
<span class="gp">'/etc/kubernetes/pki/etcd' -&gt;</span><span class="w"> </span><span class="s1">'./pki/etcd'</span>
<span class="gp">'/etc/kubernetes/pki/etcd/ca.key' -&gt;</span><span class="w"> </span><span class="s1">'./pki/etcd/ca.key'</span>
<span class="gp">'/etc/kubernetes/pki/etcd/ca.crt' -&gt;</span><span class="w"> </span><span class="s1">'./pki/etcd/ca.crt'</span>
<span class="gp">'/etc/kubernetes/pki/etcd/server.key' -&gt;</span><span class="w"> </span><span class="s1">'./pki/etcd/server.key'</span>
<span class="gp">'/etc/kubernetes/pki/etcd/server.crt' -&gt;</span><span class="w"> </span><span class="s1">'./pki/etcd/server.crt'</span>
<span class="gp">'/etc/kubernetes/pki/etcd/peer.key' -&gt;</span><span class="w"> </span><span class="s1">'./pki/etcd/peer.key'</span>
<span class="gp">'/etc/kubernetes/pki/etcd/peer.crt' -&gt;</span><span class="w"> </span><span class="s1">'./pki/etcd/peer.crt'</span>
<span class="gp">'/etc/kubernetes/pki/etcd/healthcheck-client.key' -&gt;</span><span class="w"> </span><span class="s1">'./pki/etcd/healthcheck-client.key'</span>
<span class="gp">'/etc/kubernetes/pki/etcd/healthcheck-client.crt' -&gt;</span><span class="w"> </span><span class="s1">'./pki/etcd/healthcheck-client.crt'</span>
<span class="gp">'/etc/kubernetes/pki/apiserver-etcd-client.key' -&gt;</span><span class="w"> </span><span class="s1">'./pki/apiserver-etcd-client.key'</span>
<span class="gp">'/etc/kubernetes/pki/apiserver-etcd-client.crt' -&gt;</span><span class="w"> </span><span class="s1">'./pki/apiserver-etcd-client.crt'</span>
<span class="gp">'/etc/kubernetes/pki/sa.key' -&gt;</span><span class="w"> </span><span class="s1">'./pki/sa.key'</span>
<span class="gp">'/etc/kubernetes/pki/sa.pub' -&gt;</span><span class="w"> </span><span class="s1">'./pki/sa.pub'</span>
<span class="go">    </span></code></pre></figure>

<h5 id="step-2-download-the-etcdctl-binary">Step 2: Download the etcdctl binary</h5>

<p>Download the etcdctl binary. I have created a shortlinks for the etcd-v3.2.28 which works in ubuntu16.04 and kubernetes v16.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/backup$</span><span class="w"> </span>wget shortlinks.vikki.in/etcd
<span class="go">--2019-11-26 22:05:52-- http://shortlinks.vikki.in/etcd
Resolving shortlinks.vikki.in (shortlinks.vikki.in)... 172.217.160.179, 2404:6800:4007:80d::2013
Connecting to shortlinks.vikki.in (shortlinks.vikki.in)|172.217.160.179|:80... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://github.com/vignesh88/blog/raw/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz [following]
--2019-11-26 22:05:53-- https://github.com/vignesh88/blog/raw/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz
Resolving github.com (github.com)... 13.234.176.102
Connecting to github.com (github.com)|13.234.176.102|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://raw.githubusercontent.com/vignesh88/blog/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz [following]
--2019-11-26 22:05:54-- https://raw.githubusercontent.com/vignesh88/blog/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 10529149 (10M) [application/octet-stream]
Saving to: ‘etcd’

</span><span class="gp">etcd 100%[=============================================================================&gt;</span><span class="o">]</span> 10.04M 4.47MB/s <span class="k">in </span>2.2s    
<span class="go">
2019-11-26 22:05:57 (4.47 MB/s) - ‘etcd’ saved [10529149/10529149]</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/backup$</span><span class="w"> </span><span class="nb">ls</span>
<span class="go">etcd pki</span></code></pre></figure>

<p>Extract the etcd package</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/backup$</span><span class="w"> </span><span class="nb">tar</span> <span class="nt">-xvf</span> etcd
<span class="go">etcd-v3.2.28-linux-amd64/
etcd-v3.2.28-linux-amd64/etcdctl
etcd-v3.2.28-linux-amd64/etcd
etcd-v3.2.28-linux-amd64/README-etcdctl.md
etcd-v3.2.28-linux-amd64/README.md
etcd-v3.2.28-linux-amd64/Documentation/
etcd-v3.2.28-linux-amd64/Documentation/faq.md
etcd-v3.2.28-linux-amd64/Documentation/tuning.md
etcd-v3.2.28-linux-amd64/Documentation/dl_build.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-benchmarks.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-3-demo-benchmarks.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-storage-memory-benchmark.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/_index.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/README.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-1-0-alpha-benchmarks.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-2-0-benchmarks.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-3-watch-memory-benchmark.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/
etcd-v3.2.28-linux-amd64/Documentation/op-guide/security.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/authentication.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/recovery.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/container.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/supported-platform.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/monitoring.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/clustering.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/etcd3_alert.rules
etcd-v3.2.28-linux-amd64/Documentation/op-guide/grafana.json
etcd-v3.2.28-linux-amd64/Documentation/op-guide/_index.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/versioning.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/maintenance.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/gateway.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/runtime-configuration.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/performance.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/v2-migration.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/configuration.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/etcd-sample-grafana.png
etcd-v3.2.28-linux-amd64/Documentation/op-guide/hardware.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/grpc_proxy.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/runtime-reconf-design.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/failures.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/api_concurrency_reference_v3.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/experimental_apis.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/api_grpc_gateway.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/_index.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/interacting_v3.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/grpc_naming.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/local_cluster.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/api_reference_v3.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/limit.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/v3lock.swagger.json
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/rpc.swagger.json
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/v3election.swagger.json
etcd-v3.2.28-linux-amd64/Documentation/_index.md
etcd-v3.2.28-linux-amd64/Documentation/README.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_2.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_1.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrading-etcd.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/_index.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_4.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_0.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_3.md
etcd-v3.2.28-linux-amd64/Documentation/reporting_bugs.md
etcd-v3.2.28-linux-amd64/Documentation/production-users.md
etcd-v3.2.28-linux-amd64/Documentation/branch_management.md
etcd-v3.2.28-linux-amd64/Documentation/platforms/
etcd-v3.2.28-linux-amd64/Documentation/platforms/aws.md
etcd-v3.2.28-linux-amd64/Documentation/platforms/_index.md
etcd-v3.2.28-linux-amd64/Documentation/platforms/freebsd.md
etcd-v3.2.28-linux-amd64/Documentation/platforms/container-linux-systemd.md
etcd-v3.2.28-linux-amd64/Documentation/demo.md
etcd-v3.2.28-linux-amd64/Documentation/dev-internal/
etcd-v3.2.28-linux-amd64/Documentation/dev-internal/logging.md
etcd-v3.2.28-linux-amd64/Documentation/dev-internal/discovery_protocol.md
etcd-v3.2.28-linux-amd64/Documentation/dev-internal/_index.md
etcd-v3.2.28-linux-amd64/Documentation/dev-internal/release.md
etcd-v3.2.28-linux-amd64/Documentation/learning/
etcd-v3.2.28-linux-amd64/Documentation/learning/api_guarantees.md
etcd-v3.2.28-linux-amd64/Documentation/learning/glossary.md
etcd-v3.2.28-linux-amd64/Documentation/learning/why.md
etcd-v3.2.28-linux-amd64/Documentation/learning/auth_design.md
etcd-v3.2.28-linux-amd64/Documentation/learning/data_model.md
etcd-v3.2.28-linux-amd64/Documentation/learning/api.md
etcd-v3.2.28-linux-amd64/Documentation/docs.md
etcd-v3.2.28-linux-amd64/Documentation/integrations.md
etcd-v3.2.28-linux-amd64/Documentation/rfc/
etcd-v3.2.28-linux-amd64/Documentation/rfc/_index.md
etcd-v3.2.28-linux-amd64/Documentation/rfc/v3api.md
etcd-v3.2.28-linux-amd64/Documentation/metrics.md
etcd-v3.2.28-linux-amd64/READMEv2-etcdctl.md</span></code></pre></figure>

<p>Navigate to the extraced directory</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/backup$</span><span class="w"> </span><span class="nb">ls</span>
<span class="go">etcd etcd-v3.2.28-linux-amd64 pki</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/backup$</span><span class="w"> </span><span class="nb">cd </span>etcd-v3.2.28-linux-amd64/</code></pre></figure>

<h5 id="step-3-backup-the-etcd-database">Step 3: Backup the etcd database</h5>

<p>Now you will see a etcdctl binary inside the directy.</p>

<p>Use the key, certificate and CA certificate to take backup of the existing etcd database as shown below</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/backup/etcd-v3.2.28-linux-amd64$</span><span class="w"> </span><span class="nb">sudo </span><span class="nv">ETCDCTL_API</span><span class="o">=</span>3 ./etcdctl <span class="nt">--endpoints</span> https://127.0.0.1:2379 <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/server.crt <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/server.key <span class="nt">--cacert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/ca.crt snapshot save ../etc_database_backup.db
<span class="go">Snapshot saved at ../etc_database_backup.db</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/backup/etcd-v3.2.28-linux-amd64$</span><span class="w"> </span><span class="nb">cd</span> ..
<span class="gp">vikki@kubernetes1:~/backup$</span><span class="w"> </span><span class="nb">ls</span>
<span class="go">etcd etc_database_backup.db etcd-v3.2.28-linux-amd64 pki</span></code></pre></figure>

<blockquote>
  <p>Now we can see the backup has been taken and saved as etc_database_backup.db</p>
</blockquote>]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[Kubernetes cluster state is saved in etcd datastore. In the post we are going to see how to take a backup for etcd database in kubernetes cluster.]]></summary></entry><entry><title type="html">RBAC in kubernetes</title><link href="http://0.0.0.0:4000/rbac-in-kubernetes/" rel="alternate" type="text/html" title="RBAC in kubernetes" /><published>2019-11-28T16:26:19+00:00</published><updated>2019-11-28T16:26:19+00:00</updated><id>http://0.0.0.0:4000/rbac-in-kubernetes</id><content type="html" xml:base="http://0.0.0.0:4000/rbac-in-kubernetes/"><![CDATA[<p>There are 3 elements involved in RBAC. In this post we are going to see how to provide user level access to resources.</p>

<ul>
  <li>Subjects - Users or Process that wants access to Kubernetes API</li>
  <li>Resources - Kubernetes API objects like pods, deployments etc</li>
  <li>Verbs - Set of operations like get, watch create etc</li>
</ul>

<h3 id="setup"><strong>Setup</strong></h3>

<p>I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.</p>

<!--kg-card-begin: image-->
<figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-6.jpg" class="kg-image" /></figure>
<!--kg-card-end: image-->
<h5 id="step-1-create-a-private-key">Step 1: Create a private key</h5>

<p>Create a new directly and navigate to the directory</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span><span class="nb">mkdir </span>ssl
<span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span><span class="nb">cd </span>ssl/</code></pre></figure>

<p>Use openssl command a generate a private key <em>user1.key</em></p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>openssl genrsa <span class="nt">-out</span> user1.key 2048
<span class="go">Generating RSA private key, 2048 bit long modulus
.......................................................+++
.......................................................................................................................................+++
e is 65537 (0x10001)
</span><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span><span class="nb">ls</span>
<span class="go">user1.key</span></code></pre></figure>

<h5 id="step-2-generate-a-csr">Step 2: Generate a CSR</h5>

<p>Use the private key generated in the privous step and generate the certificate signing request(csr)</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>openssl req <span class="nt">-new</span> <span class="nt">-key</span> user1.key <span class="nt">-out</span> user1.csr <span class="nt">-subj</span> <span class="s2">"/CN=user1/O=vikki.in"</span>
<span class="go">
</span><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span><span class="nb">ls</span>
<span class="go">user1.csr user1.key</span></code></pre></figure>

<h5 id="step-3-sign-the-csr-and-generate-certificate">Step 3: Sign the CSR and generate certificate</h5>

<p>The kubernetes cluster have the CA(certificate authority) key and certificate available under /etc/kubernetes/pki location</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span><span class="nb">ls</span> /etc/kubernetes/pki/ca.
<span class="go">ca.crt ca.key  </span></code></pre></figure>

<p>Use the CA certificate and key to sign the CSR</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span><span class="nb">sudo </span>openssl x509 <span class="nt">-req</span> <span class="nt">-in</span> user1.csr <span class="nt">-CA</span> /etc/kubernetes/pki/ca.crt <span class="nt">-CAkey</span> /etc/kubernetes/pki/ca.key <span class="nt">-CAcreateserial</span> <span class="nt">-out</span> user1.crt <span class="nt">-days</span> 365
<span class="go">[sudo] password for vikki: 
Signature ok
subject=/CN=user1/O=vikki.in
Getting CA Private Key

</span><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span><span class="nb">ls</span>
<span class="go">user1.crt user1.csr user1.key</span></code></pre></figure>

<blockquote>
  <p>Now we have the private key <em>user1.key</em> and signed certificate <em>user1.crt</em></p>
</blockquote>

<h5 id="step-4-set-credential-for-the-user">Step 4: Set credential for the user</h5>

<p>Now set credential for the user <em>user1</em> with the private key and the signed certificate.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl config set-credentials user1 <span class="nt">--client-certificate</span><span class="o">=</span>user1.crt <span class="nt">--client-key</span><span class="o">=</span>user1.key 
<span class="go">User "user1" set.</span></code></pre></figure>

<h5 id="step-5-set-context-for-the-user">Step 5: Set context for the user</h5>

<p>Now set the new context with the username , cluster etc.</p>

<p>We can also map the context to specific namespace using the –namespacec option. By default it will take the default namespace</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl config set-context user1-context <span class="nt">--cluster</span><span class="o">=</span>kubernetes <span class="nt">--user</span><span class="o">=</span>user1
<span class="go">Context "user1-context" created.

</span><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl config get-contexts 
<span class="go">CURRENT NAME CLUSTER AUTHINFO NAMESPACE
* kubernetes-admin@kubernetes kubernetes kubernetes-admin   
            user1-context kubernetes user1              </span></code></pre></figure>

<blockquote>
  <p>Now we can see there are 2 context created.</p>
</blockquote>

<h5 id="step-6-create-a-role">Step 6: Create a role</h5>

<p>Create a role and map the resources and verb required.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl create role myrole <span class="nt">--verb</span><span class="o">=</span>get,create,list <span class="nt">--resource</span><span class="o">=</span>pods
<span class="go">role.rbac.authorization.k8s.io/myrole created</span></code></pre></figure>

<h5 id="step-7-create-a-rolebinding">Step 7: Create a rolebinding</h5>

<p>Create a rolebinding and map the role and user.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl create rolebinding myrolebinding <span class="nt">--role</span><span class="o">=</span>myrole <span class="nt">--user</span><span class="o">=</span>user1 
<span class="go">rolebinding.rbac.authorization.k8s.io/myrolebinding created</span></code></pre></figure>

<h5 id="step-8-verify-role-and-rolebinding">Step 8: Verify role and rolebinding</h5>

<p>List the role and rolebinding and verify both are created.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl get role
<span class="go">NAME AGE
myrole 58s
</span><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl get rolebindings.rbac.authorization.k8s.io 
<span class="go">NAME AGE
myrolebinding 8s</span></code></pre></figure>

<h5 id="step-9-change-the-context-and-verify-role-based-access">Step 9: Change the context and verify role based access</h5>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl config get-contexts 
<span class="go">CURRENT NAME CLUSTER AUTHINFO NAMESPACE
* kubernetes-admin@kubernetes kubernetes kubernetes-admin   
            user1-context kubernetes user1     </span></code></pre></figure>

<p>Switch to newly created contesxt <em>user1-context</em></p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl config use-context user1-context 
<span class="go">Switched to context "user1-context".

</span><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl config get-contexts 
<span class="go">CURRENT NAME CLUSTER AUTHINFO NAMESPACE
            kubernetes-admin@kubernetes kubernetes kubernetes-admin   
* user1-context kubernetes user1           </span></code></pre></figure>

<blockquote>
  <p>Now we can see the current context is switched to <em>user1-context.</em></p>
</blockquote>

<p>Try to create a deployement</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl run nginx-deployment <span class="nt">--image</span><span class="o">=</span>nginx
<span class="go">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
Error from server (Forbidden): deployments.apps is forbidden: User "user1" cannot create resource "deployments" in API group "apps" in the namespace "default"</span></code></pre></figure>

<blockquote>
  <p>The deployment creation is failed because the new context has resource mapped only for pod.</p>
</blockquote>

<p>Now lets create a pod and verify the status</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>vim pod.yaml</code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/aa1503f5161a453f120ab6b121f6325a.js"></script>
<!--kg-card-end: html-->
<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl create <span class="nt">-f</span> pod.yaml 
<span class="go">pod/myapp-pod created</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~/ssl$</span><span class="w"> </span>kubectl get pods
<span class="go">NAME READY STATUS RESTARTS AGE
httpd-7765f5994-vc2j5 1/1 Running 1 2d
myapp-pod 0/1 ContainerCreating 0 5s
nginx-7bffc778db-p4ff5 1/1 Running 1 2d</span></code></pre></figure>

<blockquote>
  <p>We can see now the pod created successfully and running in the new context.</p>
</blockquote>

<p>We can also test the permission of user using the below command</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl auth can-i create deployments <span class="nt">--as</span> user1
<span class="go">no

</span><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl auth can-i create pods <span class="nt">--as</span> user1
<span class="go">yes</span></code></pre></figure>]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[There are 3 elements involved in RBAC. In this post we are going to see how to provide user level access to resources.]]></summary></entry><entry><title type="html">Upgrading kubernetes cluster master and worker nodes</title><link href="http://0.0.0.0:4000/upgrading-kubernetes-cluster-master-and-worker-nodes/" rel="alternate" type="text/html" title="Upgrading kubernetes cluster master and worker nodes" /><published>2019-11-26T15:07:58+00:00</published><updated>2019-11-26T15:07:58+00:00</updated><id>http://0.0.0.0:4000/upgrading-kubernetes-cluster-master-and-worker-nodes</id><content type="html" xml:base="http://0.0.0.0:4000/upgrading-kubernetes-cluster-master-and-worker-nodes/"><![CDATA[<p>This post we are going to discuss how to upgrade the kubernetes cluster, both master and worker nodes. We are going to upgrade a older version v1.15 to v.1.16.</p>

<h3 id="setup"><strong>Setup</strong></h3>

<p>I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.</p>

<!--kg-card-begin: image-->
<figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-5.jpg" class="kg-image" /></figure>
<!--kg-card-end: image-->
<h3 id="master-node">Master node</h3>

<h5 id="step-1-verify-the-current-version-of-kubelet-and-kubeadm-running-in-all-nodes">Step 1: Verify the current version of kubelet and kubeadm running in all nodes</h5>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get nodes
<span class="go">NAME STATUS ROLES AGE VERSION
kubernetes1 Ready master 41d v1.15.5
</span><span class="gp">kubernetes2 Ready &lt;none&gt;</span><span class="w"> </span>41d v1.15.5</code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubeadm version
<span class="go">kubeadm version: &amp;version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.5", GitCommit:"20c265fef0741dd71a66480e35bd69f18351daea", GitTreeState:"clean", BuildDate:"2019-10-15T19:14:19Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"linux/amd64"}</span></code></pre></figure>

<blockquote>
  <p>We can see kubelet and kubeadm is running in version v1.15.5</p>
</blockquote>

<h5 id="step-2-verify-the-lastest-stable-availble-version">Step 2: Verify the lastest stable availble version</h5>

<p>Run the update in kubernetes master node</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes1:~#</span><span class="w"> </span>apt update</code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes1:~#</span><span class="w"> </span>apt-cache policy kubeadm
<span class="go">kubeadm:
    Installed: 1.15.5-00
    Candidate: 1.16.3-00
    Version table:
</span><span class="c">    .....</span></code></pre></figure>

<blockquote>
  <p>We can see from this output that current version is 1.15.5 and the latest available is 1.16.3</p>
</blockquote>

<h5 id="step-3-upgrade-the-kubeadm-to-latest-version">Step 3: Upgrade the kubeadm to latest version</h5>

<p>Upgrade the kubeadm in master to the latest version 1.16.3</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes1:~#</span><span class="w"> </span>apt-mark unhold kubeadm <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="gp">&gt;</span><span class="w"> </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="nv">kubeadm</span><span class="o">=</span>1.16.3-00 <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="gp">&gt;</span><span class="w"> </span>apt-mark hold kubeadm
<span class="go">kubeadm was already not hold.
Hit:1 http://us.archive.ubuntu.com/ubuntu xenial InRelease                  
Hit:2 http://security.ubuntu.com/ubuntu xenial-security InRelease
Hit:3 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease
Hit:4 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease
Hit:5 https://apt.kubernetes.io kubernetes-xenial InRelease
Reading package lists... Done
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
    kubeadm
1 upgraded, 0 newly installed, 0 to remove and 142 not upgraded.
Need to get 8,762 kB of archives.
After this operation, 4,062 kB of additional disk space will be used.
Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubeadm amd64 1.16.3-00 [8,762 kB]
Fetched 8,762 kB in 7s (1,117 kB/s)                                                                                                                          
(Reading database ... 60808 files and directories currently installed.)
Preparing to unpack .../kubeadm_1.16.3-00_amd64.deb ...
Unpacking kubeadm (1.16.3-00) over (1.15.5-00) ...
Setting up kubeadm (1.16.3-00) ...
kubeadm set on hold.</span></code></pre></figure>

<h5 id="step-4-run-the-upgrade-plan">Step 4: Run the upgrade plan</h5>

<p>Now in the previous step we already updated the kubeadm to latest version. Now lets run the upgrade plan and see the available options.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes1:~#</span><span class="w"> </span><span class="nb">sudo </span>kubeadm upgrade plan
<span class="go">[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.15.5
[upgrade/versions] kubeadm version: v1.16.3
[upgrade/versions] Latest stable version: v1.16.3
[upgrade/versions] Latest version in the v1.15 series: v1.15.6

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT CURRENT AVAILABLE
Kubelet 2 x v1.15.5 v1.15.6

Upgrade to the latest version in the v1.15 series:

COMPONENT CURRENT AVAILABLE
API Server v1.15.5 v1.15.6
Controller Manager v1.15.5 v1.15.6
Scheduler v1.15.5 v1.15.6
Kube Proxy v1.15.5 v1.15.6
CoreDNS 1.3.1 1.6.2
Etcd 3.3.10 3.3.10

You can now apply the upgrade by executing the following command:

    kubeadm upgrade apply v1.15.6

_____________________________________________________________________

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT CURRENT AVAILABLE
Kubelet 2 x v1.15.5 v1.16.3

Upgrade to the latest stable version:

COMPONENT CURRENT AVAILABLE
API Server v1.15.5 v1.16.3
Controller Manager v1.15.5 v1.16.3
Scheduler v1.15.5 v1.16.3
Kube Proxy v1.15.5 v1.16.3
CoreDNS 1.3.1 1.6.2
Etcd 3.3.10 3.3.15-0

You can now apply the upgrade by executing the following command:

    kubeadm upgrade apply v1.16.3

_____________________________________________________________________</span></code></pre></figure>

<blockquote>
  <p>We can see that we can upgrade the cluster to v1.16.3 from the above output</p>
</blockquote>

<h5 id="step-5-drain-the-pods-in-master-node">Step 5: Drain the pods in master node</h5>

<p>Now drain all the pods execpt daemonsets in the master node</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl drain kubernetes1 <span class="nt">--ignore-daemonsets</span>
<span class="go">node/kubernetes1 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-bfr9j, kube-system/kube-proxy-8267n
evicting pod "coredns-5c98db65d4-v4cms"
evicting pod "coredns-5c98db65d4-g4gv4"
pod/coredns-5c98db65d4-v4cms evicted
pod/coredns-5c98db65d4-g4gv4 evicted
node/kubernetes1 evicted</span></code></pre></figure>

<h5 id="step-6-run-the-upgrade">Step 6: Run the upgrade</h5>

<p>Now run the upgrade to the lastest version 1.16.3</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span><span class="nb">sudo </span>kubeadm upgrade apply v1.16.3
<span class="go">[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/version] You have chosen to change the cluster version to "v1.16.3"
[upgrade/versions] Cluster version: v1.15.5
[upgrade/versions] kubeadm version: v1.16.3
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd]
[upgrade/prepull] Prepulling image for component etcd.
[upgrade/prepull] Prepulling image for component kube-apiserver.
[upgrade/prepull] Prepulling image for component kube-controller-manager.
[upgrade/prepull] Prepulling image for component kube-scheduler.
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[upgrade/prepull] Prepulled image for component etcd.
[upgrade/prepull] Prepulled image for component kube-apiserver.
[upgrade/prepull] Prepulled image for component kube-controller-manager.
[upgrade/prepull] Prepulled image for component kube-scheduler.
[upgrade/prepull] Successfully prepulled the images for all the control plane components
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.16.3"...
Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e
Static pod: kube-controller-manager-kubernetes1 hash: f106f0d94b93b77e4db974b1c477d277
Static pod: kube-scheduler-kubernetes1 hash: 131c3f63daec7c0750818f64a2f75d20
[upgrade/etcd] Upgrading to TLS for etcd
Static pod: etcd-kubernetes1 hash: 352a2620657e49493504dc8f27c83195
[upgrade/staticpods] Preparing for "etcd" upgrade
[upgrade/staticpods] Renewing etcd-server certificate
[upgrade/staticpods] Renewing etcd-peer certificate
[upgrade/staticpods] Renewing etcd-healthcheck-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/etcd.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/etcd.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: etcd-kubernetes1 hash: 352a2620657e49493504dc8f27c83195
Static pod: etcd-kubernetes1 hash: 3ce412f7cfe0c06939809c93f738e798
[apiclient] Found 1 Pods for label selector component=etcd
[upgrade/staticpods] Component "etcd" upgraded successfully!
[upgrade/etcd] Waiting for etcd to become available
[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests001214141"
[upgrade/staticpods] Preparing for "kube-apiserver" upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/kube-apiserver.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e
Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e
Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e
Static pod: kube-apiserver-kubernetes1 hash: d8708cb2f25dd11bbb41dd9729149325
[apiclient] Found 1 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component "kube-apiserver" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-controller-manager" upgrade
[upgrade/staticpods] Renewing controller-manager.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/kube-controller-manager.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-controller-manager-kubernetes1 hash: f106f0d94b93b77e4db974b1c477d277
Static pod: kube-controller-manager-kubernetes1 hash: e6e76bb8264f2e84070efada35e93e71
[apiclient] Found 1 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-scheduler" upgrade
[upgrade/staticpods] Renewing scheduler.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/kube-scheduler.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-scheduler-kubernetes1 hash: 131c3f63daec7c0750818f64a2f75d20
Static pod: kube-scheduler-kubernetes1 hash: 8c5e33e50bb56e8adacd1cc99c56b2cb
[apiclient] Found 1 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component "kube-scheduler" upgraded successfully!
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.16" in namespace kube-system with the configuration for the kubelets in the cluster
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.16" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.16.3". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.</span></code></pre></figure>

<blockquote>
  <p>We can see the cluster is upgraded to lastest version v1.16.3</p>
</blockquote>

<h5 id="step-7-uncordon-the-master-node">Step 7: Uncordon the master node</h5>

<p>Now ucordon the master node</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl uncordon kubernetes1 
<span class="go">node/kubernetes1 uncordoned</span></code></pre></figure>

<h5 id="step-8-upgrade-the-kubelet">Step 8: Upgrade the kubelet</h5>

<p>Now in the previous steps we have upgraded the kubeadm and cluster , next we need to upgrade the kubelet and kubectl</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes1:~#</span><span class="w"> </span>apt-mark unhold kubelet kubectl <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="gp">&gt;</span><span class="w"> </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="nv">kubelet</span><span class="o">=</span>1.16.3-00 <span class="nv">kubectl</span><span class="o">=</span>1.16.3-00 <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="gp">&gt;</span><span class="w"> </span>apt-mark hold kubelet kubectl
<span class="go">kubelet was already not hold.
kubectl was already not hold.
Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease           
Hit:2 http://us.archive.ubuntu.com/ubuntu xenial InRelease
Hit:3 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease
Hit:4 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease
Hit:5 https://apt.kubernetes.io kubernetes-xenial InRelease
Reading package lists... Done
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
    kubectl kubelet
2 upgraded, 0 newly installed, 0 to remove and 140 not upgraded.
Need to get 29.9 MB of archives.
After this operation, 7,134 kB of additional disk space will be used.
Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubectl amd64 1.16.3-00 [9,233 kB]
Get:2 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubelet amd64 1.16.3-00 [20.7 MB]                                                               
Fetched 29.9 MB in 8s (3,436 kB/s)                                                                                                                           
(Reading database ... 60808 files and directories currently installed.)
Preparing to unpack .../kubectl_1.16.3-00_amd64.deb ...
Unpacking kubectl (1.16.3-00) over (1.15.5-00) ...
Preparing to unpack .../kubelet_1.16.3-00_amd64.deb ...
Unpacking kubelet (1.16.3-00) over (1.15.5-00) ...
Setting up kubectl (1.16.3-00) ...
Setting up kubelet (1.16.3-00) ...
kubelet set on hold.
kubectl set on hold.</span></code></pre></figure>

<p>Restart the kubelet service</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes1:~#</span><span class="w"> </span><span class="nb">sudo </span>systemctl restart kubelet</code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get nodes
<span class="go">NAME STATUS ROLES AGE VERSION
kubernetes1 Ready master 41d v1.16.3
</span><span class="gp">kubernetes2 Ready &lt;none&gt;</span><span class="w"> </span>41d v1.15.5</code></pre></figure>

<blockquote>
  <p>Now we can see the master node kubernetes1 is upgraded to v1.16.3</p>
</blockquote>

<h3 id="worker-node">Worker node</h3>

<h5 id="step-1-upgrade-the-kubeadm-to-version-1163">Step 1: Upgrade the kubeadm to version 1.16.3</h5>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes2:~#</span><span class="w"> </span>apt-mark unhold kubeadm <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="gp">&gt;</span><span class="w"> </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="nv">kubeadm</span><span class="o">=</span>1.16.3-00 <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="gp">&gt;</span><span class="w"> </span>apt-mark hold kubeadm
<span class="go">kubeadm was already not hold.
Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [109 kB]                 
Get:2 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [785 kB]       
Hit:3 http://us.archive.ubuntu.com/ubuntu xenial InRelease
Get:4 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]      
Get:5 https://apt.kubernetes.io kubernetes-xenial InRelease [161 B]
Get:6 https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages [31.3 kB]                      
Get:7 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]        
Get:8 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [1,071 kB]             
Get:9 http://security.ubuntu.com/ubuntu xenial-security/main i386 Packages [617 kB]     
Get:10 http://security.ubuntu.com/ubuntu xenial-security/main Translation-en [302 kB]                                                                        
Get:11 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [466 kB]                                                                    
Get:12 http://us.archive.ubuntu.com/ubuntu xenial-updates/main i386 Packages [882 kB]                                                                        
Get:13 http://security.ubuntu.com/ubuntu xenial-security/universe i386 Packages [401 kB]                                                                     
Get:14 http://security.ubuntu.com/ubuntu xenial-security/universe Translation-en [191 kB]                                                                    
Get:15 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [5,724 B]                                                                 
Get:16 http://security.ubuntu.com/ubuntu xenial-security/multiverse i386 Packages [5,888 B]                                                                  
Get:17 http://us.archive.ubuntu.com/ubuntu xenial-updates/main Translation-en [413 kB]                                                                       
Get:18 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [771 kB]                                                                   
Get:19 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe i386 Packages [700 kB]                                                                    
Get:20 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe Translation-en [324 kB]                                                                   
Get:21 http://us.archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [16.8 kB]                                                                
Get:22 http://us.archive.ubuntu.com/ubuntu xenial-updates/multiverse i386 Packages [15.9 kB]                                                                 
Fetched 7,333 kB in 23s (315 kB/s)                                                                                                                           
Reading package lists... Done
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
    kubeadm
1 upgraded, 0 newly installed, 0 to remove and 139 not upgraded.
Need to get 8,762 kB of archives.
After this operation, 4,062 kB of additional disk space will be used.
Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubeadm amd64 1.16.3-00 [8,762 kB]
Fetched 8,762 kB in 7s (1,122 kB/s)                                                                                                                          
(Reading database ... 60808 files and directories currently installed.)
Preparing to unpack .../kubeadm_1.16.3-00_amd64.deb ...
Unpacking kubeadm (1.16.3-00) over (1.15.5-00) ...
Setting up kubeadm (1.16.3-00) ...
kubeadm set on hold.</span></code></pre></figure>

<h5 id="step-2-drain-the-pods-in-worker-node">Step 2: Drain the pods in worker node</h5>

<p>Now drain all the pods , except daemonsets from the worker node kubernetes2. <em>This command should be run in master node in case the kubenetes config is not mapped in worker</em>.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl drain kubernetes2 <span class="nt">--ignore-daemonsets</span>
<span class="go">node/kubernetes2 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-8v6lb, kube-system/kube-proxy-877b2
evicting pod "coredns-5644d7b6d9-t8qw5"
evicting pod "httpd-7765f5994-gvlj6"
evicting pod "nginx-7bffc778db-phdb5"
evicting pod "coredns-5644d7b6d9-frz4v"
pod/coredns-5644d7b6d9-frz4v evicted
pod/coredns-5644d7b6d9-t8qw5 evicted
pod/httpd-7765f5994-gvlj6 evicted
pod/nginx-7bffc778db-phdb5 evicted
node/kubernetes2 evicted</span></code></pre></figure>

<h5 id="step-3-upgrade-the-worker-node">Step 3: Upgrade the worker node</h5>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes2:~#</span><span class="w"> </span><span class="nb">sudo </span>kubeadm upgrade node
<span class="go">[upgrade] Reading configuration from the cluster...
[upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[upgrade] Skipping phase. Not a control plane node[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.16" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[upgrade] The configuration for this node was successfully updated!
[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.</span></code></pre></figure>

<h5 id="step-4-upgrade-the-kubelet-and-kubectl">Step 4: Upgrade the kubelet and kubectl</h5>

<p>In previous steps we upgraded the kubeadm and cluster. Now we need to upgrade the kubelet and kubectl.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes2:~#</span><span class="w"> </span>apt-mark unhold kubelet kubectl <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="gp">&gt;</span><span class="w"> </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="nv">kubelet</span><span class="o">=</span>1.16.3-00 <span class="nv">kubectl</span><span class="o">=</span>1.16.3-00 <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="gp">&gt;</span><span class="w"> </span>apt-mark hold kubelet kubectl
<span class="go">kubelet was already not hold.
kubectl was already not hold.
Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Hit:2 http://us.archive.ubuntu.com/ubuntu xenial InRelease                       
Hit:3 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease
Hit:4 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease
Hit:5 https://apt.kubernetes.io kubernetes-xenial InRelease
Reading package lists... Done
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
    kubectl kubelet
2 upgraded, 0 newly installed, 0 to remove and 137 not upgraded.
Need to get 29.9 MB of archives.
After this operation, 3,447 kB of additional disk space will be used.
Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubectl amd64 1.16.3-00 [9,233 kB]
Get:2 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubelet amd64 1.16.3-00 [20.7 MB]                                                               
Fetched 29.9 MB in 32s (934 kB/s)                                                                                                                            
(Reading database ... 60808 files and directories currently installed.)
Preparing to unpack .../kubectl_1.16.3-00_amd64.deb ...
Unpacking kubectl (1.16.3-00) over (1.16.2-00) ...
Preparing to unpack .../kubelet_1.16.3-00_amd64.deb ...
Unpacking kubelet (1.16.3-00) over (1.15.5-00) ...
Setting up kubectl (1.16.3-00) ...
Setting up kubelet (1.16.3-00) ...
kubelet set on hold.
kubectl set on hold.</span></code></pre></figure>

<p>Restart the kubelet service in worker node</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes2:~#</span><span class="w"> </span><span class="nb">sudo </span>systemctl restart kubelet</code></pre></figure>

<h5 id="step-5-uncordon-the-worker-node">Step 5: Uncordon the worker node</h5>

<p>Now uncordon all the pods in kubernetes2 worker onde</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl uncordon kubernetes2
<span class="go">node/kubernetes2 uncordoned</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get nodes
<span class="go">NAME STATUS ROLES AGE VERSION
kubernetes1 Ready master 41d v1.16.3
</span><span class="gp">kubernetes2 Ready &lt;none&gt;</span><span class="w"> </span>41d v1.16.3</code></pre></figure>

<blockquote>
  <p>We can see now both the master and worker nodes are upgraded from v1.15.5 to v1.16.3</p>
</blockquote>]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[This post we are going to discuss how to upgrade the kubernetes cluster, both master and worker nodes. We are going to upgrade a older version v1.15 to v.1.16.]]></summary></entry><entry><title type="html">Running kubernetes custom scheduler</title><link href="http://0.0.0.0:4000/running-kubernetes-custom-scheduler/" rel="alternate" type="text/html" title="Running kubernetes custom scheduler" /><published>2019-11-25T17:28:55+00:00</published><updated>2019-11-25T17:28:55+00:00</updated><id>http://0.0.0.0:4000/running-kubernetes-custom-scheduler</id><content type="html" xml:base="http://0.0.0.0:4000/running-kubernetes-custom-scheduler/"><![CDATA[<p>Kubernetes cluster have a default scheduler kube-scheduler. If the default scheduler does not suits our requirement we can also create our own scheduler. In the post we will discus how to create multiple scheduler and schedule pods based on different scheduler.</p>

<h3 id="setup"><strong>Setup</strong></h3>

<p>I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.</p>

<!--kg-card-begin: image-->
<figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-4.jpg" class="kg-image" /></figure>
<!--kg-card-end: image-->
<h5 id="step-1-copy-the-default-scheduler-yaml-and-modify">Step 1: Copy the default scheduler yaml and modify</h5>

<p>Copy the default scheduler yaml from master node and modify the name.</p>

<p>In this below example i am naming the custom schedule are <em>my-scheduler.</em> Add a ServiceAccount and ClusterRoleBinding to the yaml file as given below.</p>

<p>The key changes made are</p>

<ul>
  <li>line 27: name: my-scheduler</li>
  <li>line 30: serviceAccountName: my-scheduler</li>
  <li>line 38: - –leader-elect=false</li>
  <li>line 39: - –scheduler-name=my-scheduler</li>
</ul>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span><span class="nb">sudo cp</span> /etc/kubernetes/manifests/kube-scheduler.yaml custom-scheduler.yaml
<span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span><span class="nb">sudo chmod </span>777 custom-scheduler.yaml 
<span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>vim custom-scheduler.yaml </code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/2a8e765cb702a7f8edf4e8760599da10.js"></script>
<!--kg-card-end: html-->
<h5 id="step-2-create-the-custom-scheduler">Step 2: Create the custom scheduler</h5>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl create <span class="nt">-f</span> custom-scheduler.yaml 
<span class="go">serviceaccount/my-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/my-scheduler-as-kube-scheduler created
pod/my-scheduler created</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get pods <span class="nt">-n</span><span class="o">=</span>kube-system 
<span class="go">NAME READY STATUS RESTARTS AGE
calico-kube-controllers-55754f75c-fgs8g 1/1 Running 7 22d
calico-node-4h72l 1/1 Running 7 22d
calico-node-ld84s 1/1 Running 7 22d
calico-node-lrfz9 1/1 Running 2 31h
calico-node-ws576 1/1 Running 1 27h
coredns-5644d7b6d9-2g6rs 1/1 Running 7 22d
coredns-5644d7b6d9-ccxsg 1/1 Running 7 22d
etcd-kubernetes1 1/1 Running 8 22d
kube-apiserver-kubernetes1 1/1 Running 8 22d
kube-controller-manager-kubernetes1 1/1 Running 8 22d
kube-proxy-6xd8l 1/1 Running 2 31h
kube-proxy-96q5x 1/1 Running 7 22d
kube-proxy-njl6r 1/1 Running 7 22d
kube-proxy-whlhw 1/1 Running 1 27h
kube-scheduler-kubernetes1 1/1 Running 8 22d
my-scheduler 1/1 Running 0 6s</span></code></pre></figure>

<blockquote>
  <p>Now we can see a new custom scheduler <em>my-scheduler</em> is running along with defautl scheduler <em>kube-scheduler-kubernetes1</em></p>
</blockquote>

<h5 id="step-3-edit-the-custerrole-for-kube-scheduler">Step 3: Edit the custerrole for kube-scheduler</h5>

<p>If RBAC is enabled on your cluster, you must update the system:kube-scheduler cluster role. Edit the clusterrole for kube-scheduler and modify.</p>

<p>Below are the key changes made in orignal file</p>

<ul>
  <li>line 33: - my-scheduler</li>
  <li>line 131: - storageclasses</li>
</ul>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl edit clusterrole system:kube-scheduler</code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/801aa697779378ff30e46e5247de8980.js"></script>
<!--kg-card-end: html-->
<h5 id="step-4-create-pods-with-different-types-of-scheduler">Step 4: Create pods with different types of scheduler</h5>

<p>Create a pod without explicitly mentioning any scheuler name. This pod should be scheduled by default scheduler <em>kube-scheduler</em></p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>vim pod_default_scheduler.yaml</code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/cfe05dda5b00a4f170b9cdd08f6aa0dd.js"></script>
<!--kg-card-end: html-->

<p>Create a pod by explicitly mentioning custom scheuler name <em>my-scheduler</em>.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>vim pod_custom_scheduler.yaml </code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/04cc5ac3e1933b4c1979a631de424116.js"></script>
<!--kg-card-end: html-->
<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl create <span class="nt">-f</span> pod_default_scheduler.yaml 
<span class="go">pod/nginx-pod-default-scheduler created
</span><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl create <span class="nt">-f</span> pod_custom_scheduler.yaml 
<span class="go">pod/nginx-pod-custom-scheduler created</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get pods nginx-pod-custom-scheduler nginx-pod-default-scheduler
<span class="go">NAME READY STATUS RESTARTS AGE
nginx-pod-custom-scheduler 1/1 Running 0 9m9s
nginx-pod-default-scheduler 1/1 Running 0 9m13s
</span><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span></code></pre></figure>

<blockquote>
  <p>Now we can see both the pods are created and running</p>
</blockquote>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get events <span class="nt">-o</span> wide |grep nginx-pod-custom-scheduler |head <span class="nt">-1</span>
<span class="gp">&lt;unknown&gt;</span><span class="w"> </span>Normal Scheduled pod/nginx-pod-custom-scheduler my-scheduler Successfully assigned default/nginx-pod-custom-scheduler to kubernetes3 &lt;unknown&gt; 0 nginx-pod-custom-scheduler.15da76383c2c2859
<span class="go">
</span><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get events <span class="nt">-o</span> wide |grep nginx-pod-default-scheduler |head <span class="nt">-1</span>
<span class="gp">&lt;unknown&gt;</span><span class="w"> </span>Normal Scheduled pod/nginx-pod-default-scheduler default-scheduler Successfully assigned default/nginx-pod-default-scheduler to kubernetes3 &lt;unknown&gt; 0 nginx-pod-default-scheduler.15da76372f4444f7</code></pre></figure>

<blockquote>
  <p>From the events, we can see pod nginx-pod-custom-scheduler is scheduled by <em>my-scheduler</em> and nginx-pod-default-scheduler by  <em>default-scheduler.</em></p>
</blockquote>]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[Kubernetes cluster have a default scheduler kube-scheduler. If the default scheduler does not suits our requirement we can also create our own scheduler. In the post we will discus how to create multiple scheduler and schedule pods based on different scheduler.]]></summary></entry><entry><title type="html">Creating static pod in kubernetes</title><link href="http://0.0.0.0:4000/creating-static-pod-in-kubernetes/" rel="alternate" type="text/html" title="Creating static pod in kubernetes" /><published>2019-11-24T15:55:52+00:00</published><updated>2019-11-24T15:55:52+00:00</updated><id>http://0.0.0.0:4000/creating-static-pod-in-kubernetes</id><content type="html" xml:base="http://0.0.0.0:4000/creating-static-pod-in-kubernetes/"><![CDATA[<p>Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them.<br />
Static pods automatically restarts if it crashes. Static Pods are always bound to one Kubelet on a specific node. In the post we will try creating a static pod and watch the behaviour on delete.</p>

<h3 id="setup"><strong>Setup</strong></h3>

<p>I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.</p>

<!--kg-card-begin: image-->
<figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-3.jpg" class="kg-image" /></figure>
<!--kg-card-end: image-->
<h5 id="step-1-find-the-static-pod-manifest-location">Step 1: Find the static pod manifest location</h5>

<p>Go to any node where you want to run the static pod. I want to run in kubernetes3 node. Look for the kubelet process and config.yaml file associated with it</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes3:~#</span><span class="w"> </span>ps <span class="nt">-aux</span> |grep kubelet
<span class="go">root 905 3.6 9.4 544516 95676 ? Ssl 16:49 9:13 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=cgroupfs --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1
root 29041 0.0 0.0 14224 940 pts/0 S+ 21:03 0:00 grep --color=auto kubelet</span></code></pre></figure>

<p>Now grep for the staticPodPath in config file</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes3:~#</span><span class="w"> </span><span class="nb">cat</span> /var/lib/kubelet/config.yaml |grep static
<span class="go">staticPodPath: /etc/kubernetes/manifests</span></code></pre></figure>

<h5 id="step-2-go-to-the-directory-and-add-a-yaml-for-pod">Step 2: Go to the directory and add a yaml for pod</h5>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes3:~#</span><span class="w"> </span><span class="nb">cd</span> /etc/kubernetes/manifests/
<span class="gp">root@kubernetes3:/etc/kubernetes/manifests#</span><span class="w"> </span>vim static-pod.yaml</code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/bbd1584780b98d771c479a4413c97b6e.js"></script>
<!--kg-card-end: html-->

<p>Now try to grep for the pod name in the docker container list</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes3:/etc/kubernetes/manifests#</span><span class="w"> </span>docker ps <span class="nt">-a</span> |grep static-web
<span class="go">e5bf8054343a nginx "nginx -g 'daemon of…" 17 seconds ago Up 16 seconds k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0
138ea3819894 k8s.gcr.io/pause:3.1 "/pause" 23 seconds ago Up 22 seconds k8s_POD_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0</span></code></pre></figure>

<blockquote>
  <p>We can see a new static pod is created automatically</p>
</blockquote>

<h5 id="step-3-delete-the-container-and-watch-the-behaviour">Step 3: Delete the container and watch the behaviour</h5>

<p>Now lets try to delete the static pod  </p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes3:/etc/kubernetes/manifests#</span><span class="w"> </span>docker stop k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0
<span class="go">k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0
</span><span class="gp">root@kubernetes3:/etc/kubernetes/manifests#</span><span class="w"> </span>docker <span class="nb">rm </span>k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0
<span class="go">k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">root@kubernetes3:/etc/kubernetes/manifests#</span><span class="w"> </span>docker ps <span class="nt">-a</span> |grep static-web
<span class="go">d3d9713b937e nginx "nginx -g 'daemon of…" 1 second ago Up Less than a second k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_1
138ea3819894 k8s.gcr.io/pause:3.1 "/pause" About a minute ago Up About a minute k8s_POD_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0</span></code></pre></figure>

<blockquote>
  <p>We can see the new static pod is automatically creating</p>
</blockquote>]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. Static pods automatically restarts if it crashes. Static Pods are always bound to one Kubelet on a specific node. In the post we will try creating a static pod and watch the behaviour on delete.]]></summary></entry><entry><title type="html">Pod scheduling in kubernetes - detailed step by step</title><link href="http://0.0.0.0:4000/pod-scheduling-in-kubernetes-detailed-step-by-step/" rel="alternate" type="text/html" title="Pod scheduling in kubernetes - detailed step by step" /><published>2019-11-24T14:42:51+00:00</published><updated>2019-11-24T14:42:51+00:00</updated><id>http://0.0.0.0:4000/pod-scheduling-in-kubernetes-detailed-step-by-step</id><content type="html" xml:base="http://0.0.0.0:4000/pod-scheduling-in-kubernetes-detailed-step-by-step/"><![CDATA[<p>We can assign the pod to node based on various methods. Lets discuss all the below methods in the post</p>

<ul>
  <li>Using nodeName</li>
  <li>Using labels in nodeSelector</li>
  <li>Node Affinity/Anti Affinity</li>
  <li>Pod Affinity/Anti Affinity</li>
  <li>Taints and tolerations</li>
</ul>

<h3 id="setup"><strong>Setup</strong></h3>

<p>I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.</p>

<!--kg-card-begin: image-->
<figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-2.jpg" class="kg-image" /></figure>
<!--kg-card-end: image-->
<h3 id="using-nodename">Using nodeName</h3>

<h5 id="step-1-create-a-pod-and-assign-using-nodename">Step 1: Create a pod and assign using nodeName</h5>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>vim pod_node_name.yaml</code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/5f329160b5846ba72beba94bd46f8860.js"></script>
<!--kg-card-end: html-->
<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl apply <span class="nt">-f</span> pod_node_name.yaml 
<span class="go">pod/nginx-pod-nodename created</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get pods <span class="nt">-o</span> wide
<span class="go">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
</span><span class="gp">busybox 1/1 Running 4 5h47m 192.168.249.141 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">nginx-pod-nodename 1/1 Running 0 8s 192.168.80.199 kubernetes3 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-0 1/1 Running 2 5h21m 192.168.249.139 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-1 1/1 Running 2 5h21m 192.168.249.140 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-2 1/1 Running 2 5h22m 192.168.249.138 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;</code></pre></figure>

<blockquote>
  <p>Now we can see the pod is created in the node “kubernetes3” conifgued in nodeName option</p>
</blockquote>

<!--kg-card-begin: hr-->
<hr />
<!--kg-card-end: hr-->
<h3 id="using-labels-in-nodeselector">Using labels in nodeSelector</h3>

<h5 id="step-1-add-a-new-label-to-the-node">Step 1: Add a new label to the node</h5>

<p>Check the current lables using the below command</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get nodes <span class="nt">--show-labels</span> 
<span class="go">NAME STATUS ROLES AGE VERSION LABELS
kubernetes1 Ready master 20d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes1,kubernetes.io/os=linux,node-role.kubernetes.io/master=
</span><span class="gp">kubernetes2 Ready &lt;none&gt;</span><span class="w"> </span>20d v1.16.2 beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kubernetes2,kubernetes.io/os<span class="o">=</span>linux
<span class="gp">kubernetes3 Ready &lt;none&gt;</span><span class="w"> </span>117m v1.16.3 beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kubernetes3,kubernetes.io/os<span class="o">=</span>linux,namee<span class="o">=</span>node3</code></pre></figure>

<p>Add a new lable “disktype=vhd” to the kubernetes3 node</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl label nodes kubernetes3 <span class="nv">disktype</span><span class="o">=</span>vhd
<span class="go">node/kubernetes3 labeled

</span><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get nodes <span class="nt">--show-labels</span> 
<span class="go">NAME STATUS ROLES AGE VERSION LABELS
kubernetes1 Ready master 20d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes1,kubernetes.io/os=linux,node-role.kubernetes.io/master=
</span><span class="gp">kubernetes2 Ready &lt;none&gt;</span><span class="w"> </span>20d v1.16.2 beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kubernetes2,kubernetes.io/os<span class="o">=</span>linux
<span class="gp">kubernetes3 Ready &lt;none&gt;</span><span class="w"> </span>118m v1.16.3 beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,disktype<span class="o">=</span>vhd,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kubernetes3,kubernetes.io/os<span class="o">=</span>linux,namee<span class="o">=</span>node3</code></pre></figure>

<h5 id="step-2-create-a-pod-and-assign-using-nodeselector">Step 2: Create a pod and assign using nodeSelector</h5>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>vim pod_label.yaml</code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/dc26cbe554bd78b4e17103bcd96d153f.js"></script>
<!--kg-card-end: html-->
<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl create <span class="nt">-f</span> pod_label.yaml 
<span class="go">pod/nginx-pod-label created</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get pods <span class="nt">-o</span> wide
<span class="go">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
</span><span class="gp">busybox 1/1 Running 3 4h50m 192.168.249.141 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">nginx-pod-label 1/1 Running 0 4m14s 192.168.80.195 kubernetes3 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-0 1/1 Running 2 4h25m 192.168.249.139 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-1 1/1 Running 2 4h25m 192.168.249.140 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-2 1/1 Running 2 4h25m 192.168.249.138 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span></code></pre></figure>

<blockquote>
  <p>Now we can see the new pod is assinged to kubernetes3 based on nodeSelector label option</p>
</blockquote>

<!--kg-card-begin: hr-->
<hr />
<!--kg-card-end: hr-->
<h3 id="advance-pod-scheduling">Advance pod scheduling</h3>

<p>We can also assing the pod to a specific node using the Node/pod affinity and anti affinity rules.</p>

<p>Affinity types:</p>

<ul>
  <li>requiredDuringSchedulingRequiredDuringExecution</li>
  <li>requiredDuringSchedulingIgnoredDuringExecution</li>
  <li>preferredDuringSchedulingIgnoredDuringExecution</li>
</ul>

<p>Affinity operators:</p>

<ul>
  <li>In</li>
  <li>NotIn</li>
  <li>Exists</li>
  <li>DoesNotExist</li>
  <li>Gt</li>
  <li>Lt</li>
</ul>

<h3 id="node-affinityanti-affinity">Node Affinity/Anti Affinity</h3>

<h5 id="step-1-create-a-pod-with-node-affinity">Step 1: Create a pod with node affinity</h5>

<p>Create a pod with node affinity specs and match the lable using matchExpressions spec</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>vim pod_node_affinity.yaml </code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/704e4a3ce125a5e66b5b754f9edcd874.js"></script>
<!--kg-card-end: html-->
<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl create <span class="nt">-f</span> pod_node_affinity.yaml 
<span class="go">pod/nginx-pod-nodeaffinity created</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get pods
<span class="go">NAME READY STATUS RESTARTS AGE
busybox 1/1 Running 3 5h17m
nginx-pod-label 1/1 Running 0 31m
nginx-pod-nodeaffinity 0/1 Pending 0 4s
web-0 1/1 Running 2 4h52m
web-1 1/1 Running 2 4h52m
web-2 1/1 Running 2 4h52m</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl describe pods nginx-pod-nodeaffinity |grep Events: <span class="nt">-A</span> 3
<span class="go">Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
</span><span class="gp">    Warning FailedScheduling &lt;unknown&gt;</span><span class="w"> </span>default-scheduler 0/3 nodes are available: 3 node<span class="o">(</span>s<span class="o">)</span> didn<span class="s1">'t match node selector.
</span><span class="go">    </span></code></pre></figure>

<blockquote>
  <p>Now the pod is failing because there is no node has the match lables</p>
</blockquote>

<p>Lets add a label bandwidth to the kubernetes3 node</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl label nodes kubernetes3 <span class="nv">bandwidth</span><span class="o">=</span>100GB
<span class="go">node/kubernetes3 labeled</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl describe pods nginx-pod-nodeaffinity |grep Events: <span class="nt">-A</span> 5
<span class="go">Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
</span><span class="gp">    Warning FailedScheduling &lt;unknown&gt;</span><span class="w"> </span>default-scheduler 0/3 nodes are available: 3 node<span class="o">(</span>s<span class="o">)</span> didn<span class="s1">'t match node selector.
</span><span class="gp">    Warning FailedScheduling &lt;unknown&gt;</span><span class="w"> </span><span class="s1">default-scheduler 0/3 nodes are available: 3 node(s) didn'</span>t match node selector.
<span class="gp">    Normal Scheduled &lt;unknown&gt;</span><span class="w"> </span>default-scheduler Successfully assigned default/nginx-pod-nodeaffinity to kubernetes3</code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get pods <span class="nt">-o</span> wide
<span class="go">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
</span><span class="gp">busybox 1/1 Running 3 5h23m 192.168.249.141 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">nginx-pod-label 1/1 Running 0 37m 192.168.80.195 kubernetes3 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">nginx-pod-nodeaffinity 1/1 Running 0 6m16s 192.168.80.196 kubernetes3 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-0 1/1 Running 2 4h58m 192.168.249.139 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-1 1/1 Running 2 4h58m 192.168.249.140 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-2 1/1 Running 2 4h59m 192.168.249.138 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="go">    </span></code></pre></figure>

<blockquote>
  <p>Now we can see the node pod is successully assinged to the kubernetes3 node after adding the label</p>
</blockquote>

<!--kg-card-begin: hr-->
<hr />
<!--kg-card-end: hr-->
<h3 id="pod-affinityanti-affinity">Pod Affinity/Anti Affinity</h3>

<p>We can also assing the pod to a specific node using the pod affinity and anti affinity rules.</p>

<h5 id="step-1-create-a-pod-with-pod-affinity">Step 1: Create a pod with pod affinity</h5>

<p>Create a pod with pod affinity specs and match the lable using matchExpressions spec</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>vim pod_pod_affinity.yaml </code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/a4048e59449210e28e4bf66971c8d56a.js"></script>
<!--kg-card-end: html-->
<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl create <span class="nt">-f</span> pod_pod_affinity.yaml 
<span class="go">pod/nginx-pod-podaffinity created</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get pods
<span class="go">NAME READY STATUS RESTARTS AGE
busybox 1/1 Running 4 5h36m
nginx-pod-label 1/1 Running 0 49m
nginx-pod-nodeaffinity 1/1 Running 0 18m
nginx-pod-podaffinity 0/1 Pending 0 4s
web-0 1/1 Running 2 5h10m
web-1 1/1 Running 2 5h10m
web-2 1/1 Running 2 5h11m</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl describe pods nginx-pod-podaffinity |grep Events: <span class="nt">-A</span> 5 
<span class="go">Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
</span><span class="gp">    Warning FailedScheduling &lt;unknown&gt;</span><span class="w"> </span>default-scheduler 0/3 nodes are available: 1 node<span class="o">(</span>s<span class="o">)</span> had taints that the pod didn<span class="s1">'t tolerate, 2 node(s) didn'</span>t match pod affinity rules, 2 node<span class="o">(</span>s<span class="o">)</span> didn<span class="s1">'t match pod affinity/anti-affinity.
</span><span class="gp">    Warning FailedScheduling &lt;unknown&gt;</span><span class="w"> </span><span class="s1">default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn'</span>t tolerate, 2 node<span class="o">(</span>s<span class="o">)</span> didn<span class="s1">'t match pod affinity rules, 2 node(s) didn'</span>t match pod affinity/anti-affinity.</code></pre></figure>

<blockquote>
  <p>Now the pod is failing because there is no pod running in any nodes that  has the match lables</p>
</blockquote>

<p>Lets create a new pod with the label configued previously</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>vim pod_label_podaffinity.yaml</code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/09965427e66db5bc14443f49f2ed2a9d.js"></script>
<!--kg-card-end: html-->
<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl apply <span class="nt">-f</span> pod_label_podaffinity.yaml 
<span class="go">pod/nginx-pod-web-nginx-backend created</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl describe pods nginx-pod-podaffinity |grep Events: <span class="nt">-A</span> 5 
<span class="go">Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
</span><span class="gp">    Warning FailedScheduling &lt;unknown&gt;</span><span class="w"> </span>default-scheduler 0/3 nodes are available: 1 node<span class="o">(</span>s<span class="o">)</span> had taints that the pod didn<span class="s1">'t tolerate, 2 node(s) didn'</span>t match pod affinity rules, 2 node<span class="o">(</span>s<span class="o">)</span> didn<span class="s1">'t match pod affinity/anti-affinity.
</span><span class="gp">    Warning FailedScheduling &lt;unknown&gt;</span><span class="w"> </span><span class="s1">default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn'</span>t tolerate, 2 node<span class="o">(</span>s<span class="o">)</span> didn<span class="s1">'t match pod affinity rules, 2 node(s) didn'</span>t match pod affinity/anti-affinity.
<span class="gp">    Normal Scheduled &lt;unknown&gt;</span><span class="w"> </span>default-scheduler Successfully assigned default/nginx-pod-podaffinity to kubernetes3</code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get pods <span class="nt">-o</span> wide
<span class="go">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
</span><span class="gp">busybox 1/1 Running 4 5h39m 192.168.249.141 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">nginx-pod-label 1/1 Running 0 52m 192.168.80.195 kubernetes3 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">nginx-pod-nodeaffinity 1/1 Running 0 21m 192.168.80.196 kubernetes3 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">nginx-pod-podaffinity 1/1 Running 0 3m2s 192.168.80.197 kubernetes3 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">nginx-pod-web-nginx-backend 1/1 Running 0 14s 192.168.80.198 kubernetes3 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-0 1/1 Running 2 5h13m 192.168.249.139 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-1 1/1 Running 2 5h13m 192.168.249.140 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;
<span class="gp">web-2 1/1 Running 2 5h14m 192.168.249.138 kubernetes2 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;</code></pre></figure>

<blockquote>
  <p>Now we can see only the pod with lable app: web-nginx-backend is created, the previous pod is also created in the same node</p>
</blockquote>

<!--kg-card-begin: hr-->
<hr />
<!--kg-card-end: hr-->
<h3 id="taints-and-tolerations">Taints and tolerations</h3>

<p>Node affinity is a property of pods that attracts them to a set of nodes. Taints are the opposite, they allow a node to repel a set of pods.</p>

<h5 id="step-1-create-a-label-to-node-and-assign-pod-using-nodeselector">Step 1: Create a label to node and assign pod using nodeSelector</h5>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl label nodes kubernetes4 <span class="nv">app</span><span class="o">=</span>highperformance
<span class="go">node/kubernetes4 labeled
</span><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get nodes kubernetes4 <span class="nt">--show-labels</span> 
<span class="go">NAME STATUS ROLES AGE VERSION LABELS
</span><span class="gp">kubernetes4 Ready &lt;none&gt;</span><span class="w"> </span>2m55s v1.16.3 <span class="nv">app</span><span class="o">=</span>highperformance,beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/os<span class="o">=</span>linux,kubernetes.io/arch<span class="o">=</span>amd64,kubernetes.io/hostname<span class="o">=</span>kubernetes4,kubernetes.io/os<span class="o">=</span>linux</code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">    vikki@kubernetes1:~$</span><span class="w"> </span>vim pod_label_1.yaml</code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/fb98260bd5405feee85ca565b188933b.js"></script>
<!--kg-card-end: html-->
<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl create <span class="nt">-f</span> pod_label_1.yaml 
<span class="go">pod/nginx-pod-taint created</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get pod nginx-pod-taint <span class="nt">-o</span> wide
<span class="go">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
</span><span class="gp">nginx-pod-taint 1/1 Running 0 10m 192.168.48.129 kubernetes4 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;</code></pre></figure>

<blockquote>
  <p>We can see the new pod nginx-pod-taint is created and assigned to kubernetes4 node</p>
</blockquote>

<h5 id="step-2-create-a-taint">Step 2: Create a taint</h5>

<p>Now lets create a taint “NoSchedule” and add to the kubernetes4 node.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl taint nodes kubernetes4 <span class="nv">key1</span><span class="o">=</span>value1:NoSchedule
<span class="go">node/kubernetes4 tainted

</span><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get nodes kubernetes4 <span class="nt">-o</span> yaml |grep <span class="nt">-i</span> taint <span class="nt">-A</span> 3
<span class="go">    taints:
    - effect: NoSchedule
    key: key1
    value: value1</span></code></pre></figure>

<h5 id="step-3-create-a-new-pod-and-try-assign-to-kubernetes4">Step 3: Create a new pod and try assign to kubernetes4</h5>

<p>Now create a new pod and use nodeSelector to assign to kubernetes4</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>vim pod_label_2.yaml</code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/4da565b636ec481e4f6e5147b2419a93.js"></script>
<!--kg-card-end: html-->
<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl create <span class="nt">-f</span> pod_label_2.yaml 
<span class="go">pod/nginx-pod-taint-2 created</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get pods nginx-pod-taint-2 <span class="nt">-o</span> wide
<span class="go">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
</span><span class="gp">nginx-pod-taint-2 0/1 Pending 0 7s &lt;none&gt;</span><span class="w"> </span>&lt;none&gt; &lt;none&gt; &lt;none&gt;</code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl describe pod nginx-pod-taint-2 |grep <span class="nt">-i</span> events: <span class="nt">-A</span> 5
<span class="go">Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
</span><span class="gp">    Warning FailedScheduling &lt;unknown&gt;</span><span class="w"> </span>default-scheduler 0/4 nodes are available: 1 node<span class="o">(</span>s<span class="o">)</span> had taints that the pod didn<span class="s1">'t tolerate, 3 node(s) didn'</span>t match node selector.
<span class="gp">    Warning FailedScheduling &lt;unknown&gt;</span><span class="w"> </span>default-scheduler 0/4 nodes are available: 1 node<span class="o">(</span>s<span class="o">)</span> had taints that the pod didn<span class="s1">'t tolerate, 3 node(s) didn'</span>t match node selector.</code></pre></figure>

<blockquote>
  <p>We can see the pod creatation if failing due to the taints setting.</p>
</blockquote>

<h5 id="step-4-update-the-pod-with-tolerations">Step 4: Update the pod with tolerations</h5>

<p>Now lets add toleration to the same pod for “NoSchedule” and apply the changes.</p>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>vim pod_label_3.yaml </code></pre></figure>
<!--kg-card-begin: html-->
<script src="https://gist.github.com/vigneshragupathy/5790929db238b9276d4961d3a91e6a29.js"></script>
<!--kg-card-end: html-->
<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl apply <span class="nt">-f</span> pod_label_3.yaml 
<span class="go">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
pod/nginx-pod-taint-2 configured</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl describe pod nginx-pod-taint-2 |grep <span class="nt">-i</span> events: <span class="nt">-A</span> 5
<span class="go">Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
</span><span class="gp">    Warning FailedScheduling &lt;unknown&gt;</span><span class="w"> </span>default-scheduler 0/4 nodes are available: 1 node<span class="o">(</span>s<span class="o">)</span> had taints that the pod didn<span class="s1">'t tolerate, 3 node(s) didn'</span>t match node selector.
<span class="gp">    Warning FailedScheduling &lt;unknown&gt;</span><span class="w"> </span>default-scheduler 0/4 nodes are available: 1 node<span class="o">(</span>s<span class="o">)</span> had taints that the pod didn<span class="s1">'t tolerate, 3 node(s) didn'</span>t match node selector.
<span class="gp">    Normal Scheduled &lt;unknown&gt;</span><span class="w"> </span>default-scheduler Successfully assigned default/nginx-pod-taint-2 to kubernetes4</code></pre></figure>

<figure class="highlight"><pre><code class="language-console" data-lang="console"><span class="gp">vikki@kubernetes1:~$</span><span class="w"> </span>kubectl get pod nginx-pod-taint-2 <span class="nt">-o</span> wide
<span class="go">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
</span><span class="gp">nginx-pod-taint-2 1/1 Running 0 11m 192.168.48.130 kubernetes4 &lt;none&gt;</span><span class="w"> </span>&lt;none&gt;</code></pre></figure>

<blockquote>
  <p>Now we can see the pod changed from failed to success state and assinged to kubernetes4 node according to the nodeSelector.</p>
</blockquote>]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[We can assign the pod to node based on various methods. Lets discuss all the below methods in the post]]></summary></entry></feed>