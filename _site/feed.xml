<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-08-01T07:09:30+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Vignesh Ragupathy</title><subtitle>personal description</subtitle><author><name>Vignesh Ragupathy</name><email>r.vignesh88@gmail.com</email></author><entry><title type="html">Kubernetes monitoring in Zabbix via Prometheus backend</title><link href="http://0.0.0.0:4000/kubernetes-monitoring-in-zabbix-via-prometheus-backend/" rel="alternate" type="text/html" title="Kubernetes monitoring in Zabbix via Prometheus backend" /><published>2022-07-01T10:00:00+00:00</published><updated>2022-07-01T10:00:00+00:00</updated><id>http://0.0.0.0:4000/kubernetes-monitoring-in-zabbix-via-prometheus-backend</id><content type="html" xml:base="http://0.0.0.0:4000/kubernetes-monitoring-in-zabbix-via-prometheus-backend/"><![CDATA[## Summary

Monitoring in Kubernetes is a complex task.

The traditional monitoring framework is not sufficient to handle such a massive workload.

Zabbix since version 6.0 provides a native way of integration for monitoring Kubernetes cluster.

Zabbix-Kubernetes integration provides various templates to monitor kubernetes components like `kube-controller-manager`, `kube-apiserver`, `kube-scheduler`, `kubelet`, etc.

It also supports automatic discovery of kubernetes nodes, pods and also collects metrics agentlessly.

## Why I don't like the Zabbix's direct way of monitoring Kubernetes cluster?

Although Zabbix-Kubernetes integration looks promising in the beginning , it is not easy to use.

The documentation is not clear, it is mostly based on the assumption that the Zabbix server is running inside the same Kubernetes cluster.

There are so much details missing in the documentation especially if you want to do the monitoring from external Zabbix server or even to do a multiple cross cluster monitoring.

And finally, most of all, the  Zabbix monitoring system is not designed to monitor `directly` such a volatile, dynamic, multi-dimensional metrics from infrastructure like Kubernetes.

## Why Prometheus?

Prometheus is an opensource monitoring system which can collect massive amount of data in near real-time using it's pull-based mechanism.

Prometheus can be easily configured for service discovery.

It is agentless and works by sending an HTTP request so-called scrape, which can find the endpoints and scrap metrics from any source.

The response from scrape request is parsed and stored in Prometheus database.

Prometheus provided powerful querying using it's PromQL and the metrics collected can be pulled from any external system like Zabbix or Grafana via its API.

> Using Prometheus federation, we can easily collect metrics from multiple Kubernetes clusters and create a hierarchically scaled monitoring system.

## How Prometheus exposes metrics?

Prometheus collects the metrics from any source by scraping the HTTP endpoint which contains metrics.

It also provides a web interface to view the metrics.

The metrics are exposed via API.

Prometheus metrics can be queried by any HTTP client using it's API Endpoints.

**Query Prometheus using curl**

```
curl -X GET  'https://PROMETHEUS_HOSTNAME/api/v1/query?query=up' | jq status.
```

**Query on kubernetes node metrics**

```
curl -X GET  'https://PROMETHEUS_HOSTNAME/api/v1/query?query=kube_node_info' | jq status.
```

**Advanced query operations like boolean**

```
curl -X GET  'http://PROMETHEUS_HOSTNAME/api/v1/query?query={__name__=~"kube_pod_container_resource_limits_cpu_cores|kube_pod_status_phase"}>0' | jq .
```

## Why Prometheus is just not good enough?

Prometheus is good for collecting the metrics from Kubernetes cluster.
But it is not enough to act as a full-stack monitoring system.

Here are some of the limitations.

- Very basic visualization of the metrics.
- No AI/ML features like automatic anomaly detection
- No role based access control
- Long term storage is not available.

## Proposed approach

> The proposed approach is to use Prometheus as a backend for Zabbix.

This approach provides the flexibility of using Prometheus pull based metrics collection, scalability and PromQL queries. Also the advantage of Zabbix for alerting, anomaly detection and role based access control etc.

**Architecture of the proposed approach**

![prometheus_federate](/content/images/2022/kubernetes-promethues-federate.png)]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="opensource" /><category term="observability" /><summary type="html"><![CDATA[Summary]]></summary></entry><entry><title type="html">My life at SBI</title><link href="http://0.0.0.0:4000/lifeatsbi/" rel="alternate" type="text/html" title="My life at SBI" /><published>2021-09-16T12:15:00+00:00</published><updated>2021-09-16T12:15:00+00:00</updated><id>http://0.0.0.0:4000/lifeatsbi</id><content type="html" xml:base="http://0.0.0.0:4000/lifeatsbi/"><![CDATA[## Summary 

I joined State Bank of India on July 2020 and worked for 1 year and 2 months as Manager - IT Infrastructure Architect in Enterprise and Technology Architecture department in GITC, Navi Mumbai. 

My role in SBI is technical, and it involves consulting, design, Architecture and reviewing of application and infrastructure. 

Through out my tenure in SBI i worked on various short term projects. Among them setting up DevOps infrastructure for SBI is one.  

It’s an ongoing project and we’re constantly finding ways to improve and make life easier for developers and Operations team across SBI. 

Breakup of overall time that had spent in SBI.

![sbi_breadkdown](/content/images/2021/sbi_work_breakup.png)


## What have I learned ?

-  Importance of architecture principles and policies 
-  Some software architectural design
-  Compliance and regulation in banking
-  DevOps tools and practises
-  Managing vendor partners
-  Tracking project timelines

#### Certificate of appreciation for delivering a talk on Kubernetes

<iframe src="https://www.linkedin.com/embed/feed/update/urn:li:ugcPost:6757560991862669312" height="448" width="504" frameborder="0" allowfullscreen="" title="Embedded post"></iframe>

## My timeline in SBI

- Sep 2019 - Specialist exam for SBI
- Dec 2019 - Personal Interview in Mumbai, SBI
- Jan 2020 -  Received job offer from SBI
- Feb 2020 - Pandemic and full work from home start in Ericsson
- Apr 2021 - Relieving extension in Ericsson 
- Jun 2020 - Last day in Ericsson
- Jul 2020 - Relocation to Mumbai and joining SBI
- Aug 2020 - Joining Enterprise and Technology architecture dept in SBI
- Oct 2020 - Travel to native and bringing family to Mumbai
- April 2021 - Travel to native and leaving family
- May 2021 - COVID like symptoms and home quarantine
- June 2021 - Decision to leave SBI and relocate from Mumbai
- July 2021 - New job offer received
- Aug 2021 - Resignation initiation in SBI
- Sep 2021 - Last working day in SBI

## Why leaving job from State Bank of India

The decision for leaving the job at state bank of India is both personal and family. Some of the key points that influence the decisions are no WFH, no relocation/transfer of job, micro management, unnecessary and complicated process, poor decision making and low learning curve/Legacy technologies. 

One of the important factor that any organisation should focus is equally changing the organisation culture when they modernise the technology stack. There are significant effort put on hiring technical experts, digital transformation, cloud strategy but very little on organisation culture.

## To SBI (on request from SBI colleagues)

-  Invest as equally as technology transformation in cultural transformation.
-  Let the internal team solve the hard problems and use the external teams /vendors only to enable the in-house teams
-  IT operations need engineers, not just the people managers. Replace people managers with engineering managers whereever needed. Do not mix them both. 
-  Make an open/transparent and employee friendly organisation.

## So whats next ?

So what am I doing next? It's been a privilege and an adventure to work in SBI as Infrastructure Architect with so many amazing people. But I'm now excited about my new role as Sr Site Reliability Engineer at Sage Intacct, Bangalore.]]></content><author><name>Vignesh Ragupathy</name></author><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[Summary]]></summary></entry><entry><title type="html">Plotly4Nagios - A Graph plugin for nagios monitoring</title><link href="http://0.0.0.0:4000/plotly4nagios-graph-plugin-for-nagios-monitoring/" rel="alternate" type="text/html" title="Plotly4Nagios - A Graph plugin for nagios monitoring" /><published>2021-03-24T12:15:00+00:00</published><updated>2021-03-24T12:15:00+00:00</updated><id>http://0.0.0.0:4000/plotly4nagios-graph-plugin-for-nagios-monitoring</id><content type="html" xml:base="http://0.0.0.0:4000/plotly4nagios-graph-plugin-for-nagios-monitoring/"><![CDATA[[Plotly4Nagios](https://github.com/vigneshragupathy/plotly4nagios) is a nagios plugin to display the performance data in Graph. It uses the RRD database provided by pnp4nagios and visualize it in interactive graph format using plotly javascript. The first pre-release is published today in [github](https://github.com/vigneshragupathy/plotly4nagios) and here is the installation document. You can experiment it and report the issue/feedback for further enhancement.

> Plotly4Nagios is accepted and listed under official [nagios addons](https://exchange.nagios.org/directory/Addons/Graphing-and-Trending/Plotly4Nagios/details)

## GIT badges

![GitHub](https://img.shields.io/github/license/vigneshragupathy/plotly4nagios)
[![Build Status](https://travis-ci.com/vigneshragupathy/plotly4nagios.svg?branch=main)](https://travis-ci.com/vigneshragupathy/plotly4nagios)

## Features

- Easy integration with nagios `notes_url`.
- Single page view for all performance metrics.
- Easy template change using configuration variable.
- Docker container based deploy and run.

## Prerequisite

- [pnp4nagios](https://support.nagios.com/kb/article/nagios-core-performance-graphs-using-pnp4nagios-801.html)

## Installation

- Download plotly4nagios.tar.gz and extract it under /usr/local/plotly4nagios
- Modify the config.json variables according to the environment
- Copy the plotly4nagios/plotly4nagios.conf to /etc/http/conf.d/ folder and restart httpd
- Add the follwing with  notes_url to templates.cfg.

``` bash
    notes_url /plotly4nagios/plotly4nagios.html?host=\$HOSTNAME\$&srv=_HOST_
    notes_url /plotly4nagios/plotly4nagios.html?host=\$HOSTNAME$&srv=\$SERVICEDESC$
```

- Restart httpd and nagios.

## Installation with docker(Ubuntu image)

- Build the docker image using the below command

```bash
git clone https://github.com/vigneshragupathy/plotly4nagios.git
cd plotly4nagios
docker build -t plotly4nagios .
```

- Run the docker container using the below command

```bash
docker run -it --name plotly4nagios -p 80:80 plotly4nagios
```

Alternatively direct pull and run from docker hub.

```bash
docker run -d -p 80:80 --name plotly4nagios vigneshragupathy/plotly4nagios
```

> Open from the browser and view the application at http://localhost/nagios

### Login details

- Username : nagiosadmin
- Password : nagios

## Demo

!['demo'](https://raw.githubusercontent.com/vigneshragupathy/plotly4nagios/main/img/plotly4nagios.gif)

## Screenshot

### Dark mode

!['Dark mode'](https://raw.githubusercontent.com/vigneshragupathy/plotly4nagios/main/img/screenshot_darkmode.png)

## License

Copyright 2020-2021 © Vignesh Ragupathy. All rights reserved.

Licensed under the [MIT License](https://github.com/vigneshragupathy/plotly4nagios/blob/ed09f8d687014107c8002d92acbc7acd2f62468a/LICENSE)]]></content><author><name>Vignesh Ragupathy</name></author><category term="linux" /><category term="opensource" /><category term="observability" /><summary type="html"><![CDATA[Plotly4Nagios is a nagios plugin to display the performance data in Graph. It uses the RRD database provided by pnp4nagios and visualize it in interactive graph format using plotly javascript. The first pre-release is published today in github and here is the installation document. You can experiment it and report the issue/feedback for further enhancement.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/content/images/2021/plotly4nagios_dark.png" /><media:content medium="image" url="http://0.0.0.0:4000/content/images/2021/plotly4nagios_dark.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Publish package in NPM and serve the static content from CDN</title><link href="http://0.0.0.0:4000/publish-package-in-npm-and-serve-the-static-content-from-cdn/" rel="alternate" type="text/html" title="Publish package in NPM and serve the static content from CDN" /><published>2020-06-12T12:15:00+00:00</published><updated>2020-06-12T12:15:00+00:00</updated><id>http://0.0.0.0:4000/publish-package-in-npm-and-serve-the-static-content-from-cdn</id><content type="html" xml:base="http://0.0.0.0:4000/publish-package-in-npm-and-serve-the-static-content-from-cdn/"><![CDATA[![](/content/images/cover/npm.jpg)
*Photo by [Vignesh Ragupathy](https://photography.vikki.in/vikki-photography-budapest-3){:target="_blank"}.*

I have been utilizing AWS to host my personal blog for almost 3 years now. Originally my blog was hosted in WordPress and then I migrated to [ghost](https://ghost.org/){:target="_blank"}. It's been 2 years now in ghost and I thought of exploring a new hosting option which should be free, supports custom domain name and free [SSL](https://letsencrypt.org/){:target="_blank"}.

[Jekyll](https://jekyllrb.com/){:target="_blank"} is a ruby based static blog generator and it has an advantage of free hosting in GitHub. The letsencrypt SSL certificate is also provided by GitHub for my custom domain so i don’t have to worry about managing it.

I also created a separate [website](https://tools.vikki.in){:target="_blank"} to showcase my open-source tools and I can use the same AWS instance for hosting it. It is a Django application which uses more memory/CPU, so i can run it in a dedicated instance instead of running the ghost and Django together.

One of the challenges in a Django application is hosting your static content. Django recommends using a proxy server like Nginx to serve its static content.

I use my nginx proxy to serve the static content. But due to performance reason , i started to explore the free CDN to serve my static content

Below is the nginx configuration snippet for mapping static content.

{% highlight console %}
location /static/ {
        root /tools.vikki.in/static;
    }
{% endhighlight %}

After doing some research I chose to utilize unpkg or jsdelivr for my site.

> unpkg and jsdelivr are global CDN and they can be used to deliver any packages hosted in NPM

[unpkg](https://unpkg.com/){:target="_blank"} and [jsdelivr](https://www.jsdelivr.com/){:target="_blank"} both provides CDN for the content hosted in NPM.
So first we should have the static content published in [NPM](https://www.npmjs.com/){:target="_blank"}.

## NPM Package creation

### 1. Create the directory for adding packages for NPM

{% highlight console %}
mkdir npm
mkdir npm/dist
cd npm
{% endhighlight %}

### 2. Create a package.json file for your package

{% highlight console %}
npm init
{% endhighlight %}

{% highlight console %}
This utility will walk you through creating a package.json file.
It only covers the most common items, and tries to guess sensible defaults.

See `npm help json` for definitive documentation on these fields
and exactly what they do.

Use `npm install pkg` afterwards to install a package and
save it as a dependency in the package.json file.

Press ^C at any time to quit.
package name: (npm) vikki-tools
version: (1.0.0) 1.0.7
description: Libraries for https://tools.vikki.in
entry point: (index.js) dist/index.js
test command:
git repository: https://github.com/vignesh88/tools.git
keywords: vikki, tools
author: Vignesh Ragupathy
license: (ISC)
About to write to /home/vikki/npm/package.json:

{% endhighlight %}
{% highlight json linenos %}
{
  "name": "vikki-tools",
  "version": "1.0.7",
  "description": "Libraries for https://tools.vikki.in",
  "main": "dist/index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/vignesh88/tools.git"
  },
  "keywords": [
    "vikki",
    "tools"
  ],
  "author": "Vignesh Ragupathy",
  "license": "ISC",
  "bugs": {
    "url": "https://github.com/vignesh88/tools/issues"
  },
  "homepage": "https://github.com/vignesh88/tools#readme"
}
{% endhighlight %}
{% highlight console %}
Is this OK? (yes) yes
{% endhighlight %}


### 3. Create a index.js

I added a javascript function that will be used to copy text to clipboard.

{% highlight console %}
vim dist/index.js
{% endhighlight %}

{% highlight javascript linenos %}
function copyToClipboard(x,y) {
    if( document.getElementById(x).value) {
        data_2_copy = document.getElementById(x).value;
    } else {
        data_2_copy = document.getElementById(x).innerText;
    }

    var e = document.createElement("textarea");
    e.style.opacity = "0", e.style.position = "fixed", e.textContent = data_2_copy;
    var t = document.getElementsByTagName("body")[0];
    t.appendChild(e), e.select(), document.execCommand("copy"), t.removeChild(e), $(y).show(), setTimeout((function () {
        $(y).hide()
    }), 1e3)
}
{% endhighlight %}

### 4. Copy all your static content to dist directory

Now lets copy all our static content to the <mark>dist</mark> directory.
I have various css,images,javascript that will be used in various app inside my django application.

Below are the files which i copied.

{% highlight console %}
tree .
.
├── dist
│   ├── admin
│   │   ├── css
│   │   │   ├── autocomplete.css
│   │   │   ├── base.css
│   ├── geoip
│   │   ├── css
│   │   │   ├── geoip_dark.css
│   │   │   └── geoip_light.css
│   │   └── js
│   │       └── geoip.js
│   ├── index.js
│   └── password_generator
│       ├── css
│       │   ├── password_generator_dark.css
│       │   └── password_generator_light.css
│       ├── img
│       │   ├── copy-full.svg
│       │   └── regenerate.svg
│       └── js
│           └── password_generator.js
├── package.json
└── README.md
{% endhighlight %}

### 5. Publish you static content as package in NPM

Now we are all set, let's connect to NPM and publish our package.

> You should already have an account in NPM to publish.

{% highlight console %}
npm login
Username: r_vignesh
Password: 
Email: (this IS public) me@vikki.in
Logged in as r_vignesh on https://registry.npmjs.org/.
{% endhighlight %}


{% highlight console %}
npm publish

npm notice
npm notice package: vikki-tools@1.0.7
npm notice === Tarball Contents ===
npm notice 1.1kB   dist/admin/img/LICENSE
npm notice 8.4kB   dist/admin/css/autocomplete.css
npm notice 16.4kB  dist/admin/css/base.css
npm notice 698B    dist/base64/css/base64_dark.css
npm notice 159B    dist/base64/css/base64_light.css
npm notice 85.9kB  dist/admin/fonts/Roboto-Regular-webfont.woff
npm notice === Tarball Details ===
npm notice name:          vikki-tools
npm notice version:       1.0.7
npm notice package size:  1.1 MB
npm notice unpacked size: 3.9 MB
npm notice shasum:        a9153c3a9bb68bc34d5040d2088a5b95a256e4cc
npm notice integrity:     sha512-zynWl1/pL0Wvk[...]k3yhkCzBz7+0A==
npm notice total files:   188
npm notice
+ vikki-tools@1.0.7
{% endhighlight %}

That's it. Now we have the package published in NPM.

> Unpkg and Jsdelivr provide CDN for NPM packages without any configuration.

### 6. Verify published package in NPM

Lets try to access it using unpkg. Open your browser and enter the url in the below format.

<mark>https://unpkg.com/pacakage/</mark>

For using specific version <mark>https://unpkg.com/package@version/:file</mark>

My package name is *vikki-tools* so the format will be [https://unpkg.com/vikki-tools<mark>/</mark>](https://unpkg.com/vikki-tools/){:target="_blank"}.

> The leading <mark> / </mark> at the end of the URL is important.

### Screenshot from browser

![unpkg screenshot](/content/images/2020/screenshot_vikki_tools.jpg)

## Using Unpkg to serve static content in website

We can now load the static content from NPM on our website.

{% highlight html linenos %}
<script src="https://unpkg.com/vikki-tools@1.0.3/dist/index.js"></script>
<link href="https://unpkg.com/vikki-tools@1.0.3/dist/base64/css/base64_dark.css" rel="stylesheet">
{% endhighlight %}

## Using Jsdelivr to serve static content in website

We can also use Jsdelivr instead of unpkg.

{% highlight html linenos %}
<script src="https://cdn.jsdelivr.net/npm/vikki-tools@1.0.3/dist/index.js"></script>
<link href="https://cdn.jsdelivr.net/npm/vikki-tools@1.0.3/dist/base64/css/base64_dark.css" rel="stylesheet">
{% endhighlight %}

## Auto minified version from jsdelivr

Jsdelivr also provides the auto minified version of the CSS and Javascript from NPM.
If you want to use minified version css and js, just add <mark>.min</mark> extension to the filename

{% highlight html linenos %}
<script src="https://cdn.jsdelivr.net/npm/vikki-tools@1.0.3/dist/index.min.js"></script>
<link href="https://cdn.jsdelivr.net/npm/vikki-tools@1.0.3/dist/base64/css/base64_dark.min.css" rel="stylesheet">
{% endhighlight %}

## Script to automatically update the static and CDN URL in Django

For ease, I created a script to automatically update all static content in your template directory in the Django application.

The code is available in the [Github URL](https://github.com/vignesh88/tools/blob/master/vikki_scripts/django_template_static_to_cdn.py){:target="_blank"}

## Demo video



<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://www.youtube.com/embed/PKGLkmzHQH0' frameborder='0' allowfullscreen></iframe></div>

<!--
<object style="width:100%;height:100%;width: 820px; height: 461.25px; float: none; clear: both; margin: 2px auto;" data="https://www.youtube.com/embed/XMSV5J4jxSo">
</object>
<iframe width="560" height="315" src="https://www.youtube.com/embed/XMSV5J4jxSo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
-->]]></content><author><name>Vignesh Ragupathy</name></author><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[Photo by Vignesh Ragupathy.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://0.0.0.0:4000/content/images/cover/npm.jpg" /><media:content medium="image" url="http://0.0.0.0:4000/content/images/cover/npm.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Backup of etcd database in kubernetes</title><link href="http://0.0.0.0:4000/backup-of-etcd-database-in-kubernetes/" rel="alternate" type="text/html" title="Backup of etcd database in kubernetes" /><published>2019-11-28T16:37:26+00:00</published><updated>2019-11-28T16:37:26+00:00</updated><id>http://0.0.0.0:4000/backup-of-etcd-database-in-kubernetes</id><content type="html" xml:base="http://0.0.0.0:4000/backup-of-etcd-database-in-kubernetes/"><![CDATA[Kubernetes cluster state is saved in etcd datastore. In the post we are going to see how to take a backup for etcd database in kubernetes cluster.

### **Setup**

I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.

<!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-7.jpg" class="kg-image"></figure><!--kg-card-end: image-->
##### Step 1: Create a directory and backup all certificates

Kubernetes cluster have all the certificates saved in the defautl path /etc/kubernetes/pki. Take the backup of all the files and save it in the backup directory

{% highlight console %}

vikki@kubernetes1:~$ mkdir backup 
vikki@kubernetes1:~$ cd backup/
{% endhighlight %}


{% highlight console %}

vikki@kubernetes1:~/backup$ sudo cp -rvf /etc/kubernetes/pki .
'/etc/kubernetes/pki' -> './pki'
'/etc/kubernetes/pki/ca.key' -> './pki/ca.key'
'/etc/kubernetes/pki/ca.crt' -> './pki/ca.crt'
'/etc/kubernetes/pki/apiserver.key' -> './pki/apiserver.key'
'/etc/kubernetes/pki/apiserver.crt' -> './pki/apiserver.crt'
'/etc/kubernetes/pki/apiserver-kubelet-client.key' -> './pki/apiserver-kubelet-client.key'
'/etc/kubernetes/pki/apiserver-kubelet-client.crt' -> './pki/apiserver-kubelet-client.crt'
'/etc/kubernetes/pki/front-proxy-ca.key' -> './pki/front-proxy-ca.key'
'/etc/kubernetes/pki/front-proxy-ca.crt' -> './pki/front-proxy-ca.crt'
'/etc/kubernetes/pki/front-proxy-client.key' -> './pki/front-proxy-client.key'
'/etc/kubernetes/pki/front-proxy-client.crt' -> './pki/front-proxy-client.crt'
'/etc/kubernetes/pki/etcd' -> './pki/etcd'
'/etc/kubernetes/pki/etcd/ca.key' -> './pki/etcd/ca.key'
'/etc/kubernetes/pki/etcd/ca.crt' -> './pki/etcd/ca.crt'
'/etc/kubernetes/pki/etcd/server.key' -> './pki/etcd/server.key'
'/etc/kubernetes/pki/etcd/server.crt' -> './pki/etcd/server.crt'
'/etc/kubernetes/pki/etcd/peer.key' -> './pki/etcd/peer.key'
'/etc/kubernetes/pki/etcd/peer.crt' -> './pki/etcd/peer.crt'
'/etc/kubernetes/pki/etcd/healthcheck-client.key' -> './pki/etcd/healthcheck-client.key'
'/etc/kubernetes/pki/etcd/healthcheck-client.crt' -> './pki/etcd/healthcheck-client.crt'
'/etc/kubernetes/pki/apiserver-etcd-client.key' -> './pki/apiserver-etcd-client.key'
'/etc/kubernetes/pki/apiserver-etcd-client.crt' -> './pki/apiserver-etcd-client.crt'
'/etc/kubernetes/pki/sa.key' -> './pki/sa.key'
'/etc/kubernetes/pki/sa.pub' -> './pki/sa.pub'
    

{% endhighlight %}
##### Step 2: Download the etcdctl binary

Download the etcdctl binary. I have created a shortlinks for the etcd-v3.2.28 which works in ubuntu16.04 and kubernetes v16.

{% highlight console %}

vikki@kubernetes1:~/backup$ wget shortlinks.vikki.in/etcd
--2019-11-26 22:05:52-- http://shortlinks.vikki.in/etcd
Resolving shortlinks.vikki.in (shortlinks.vikki.in)... 172.217.160.179, 2404:6800:4007:80d::2013
Connecting to shortlinks.vikki.in (shortlinks.vikki.in)|172.217.160.179|:80... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://github.com/vignesh88/blog/raw/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz [following]
--2019-11-26 22:05:53-- https://github.com/vignesh88/blog/raw/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz
Resolving github.com (github.com)... 13.234.176.102
Connecting to github.com (github.com)|13.234.176.102|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://raw.githubusercontent.com/vignesh88/blog/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz [following]
--2019-11-26 22:05:54-- https://raw.githubusercontent.com/vignesh88/blog/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 10529149 (10M) [application/octet-stream]
Saving to: ‘etcd’

etcd 100%[=============================================================================>] 10.04M 4.47MB/s in 2.2s    

2019-11-26 22:05:57 (4.47 MB/s) - ‘etcd’ saved [10529149/10529149]

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~/backup$ ls
etcd pki

{% endhighlight %}

Extract the etcd package

{% highlight console %}

vikki@kubernetes1:~/backup$ tar -xvf etcd
etcd-v3.2.28-linux-amd64/
etcd-v3.2.28-linux-amd64/etcdctl
etcd-v3.2.28-linux-amd64/etcd
etcd-v3.2.28-linux-amd64/README-etcdctl.md
etcd-v3.2.28-linux-amd64/README.md
etcd-v3.2.28-linux-amd64/Documentation/
etcd-v3.2.28-linux-amd64/Documentation/faq.md
etcd-v3.2.28-linux-amd64/Documentation/tuning.md
etcd-v3.2.28-linux-amd64/Documentation/dl_build.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-benchmarks.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-3-demo-benchmarks.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-storage-memory-benchmark.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/_index.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/README.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-1-0-alpha-benchmarks.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-2-0-benchmarks.md
etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-3-watch-memory-benchmark.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/
etcd-v3.2.28-linux-amd64/Documentation/op-guide/security.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/authentication.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/recovery.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/container.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/supported-platform.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/monitoring.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/clustering.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/etcd3_alert.rules
etcd-v3.2.28-linux-amd64/Documentation/op-guide/grafana.json
etcd-v3.2.28-linux-amd64/Documentation/op-guide/_index.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/versioning.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/maintenance.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/gateway.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/runtime-configuration.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/performance.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/v2-migration.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/configuration.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/etcd-sample-grafana.png
etcd-v3.2.28-linux-amd64/Documentation/op-guide/hardware.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/grpc_proxy.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/runtime-reconf-design.md
etcd-v3.2.28-linux-amd64/Documentation/op-guide/failures.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/api_concurrency_reference_v3.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/experimental_apis.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/api_grpc_gateway.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/_index.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/interacting_v3.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/grpc_naming.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/local_cluster.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/api_reference_v3.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/limit.md
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/v3lock.swagger.json
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/rpc.swagger.json
etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/v3election.swagger.json
etcd-v3.2.28-linux-amd64/Documentation/_index.md
etcd-v3.2.28-linux-amd64/Documentation/README.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_2.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_1.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrading-etcd.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/_index.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_4.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_0.md
etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_3.md
etcd-v3.2.28-linux-amd64/Documentation/reporting_bugs.md
etcd-v3.2.28-linux-amd64/Documentation/production-users.md
etcd-v3.2.28-linux-amd64/Documentation/branch_management.md
etcd-v3.2.28-linux-amd64/Documentation/platforms/
etcd-v3.2.28-linux-amd64/Documentation/platforms/aws.md
etcd-v3.2.28-linux-amd64/Documentation/platforms/_index.md
etcd-v3.2.28-linux-amd64/Documentation/platforms/freebsd.md
etcd-v3.2.28-linux-amd64/Documentation/platforms/container-linux-systemd.md
etcd-v3.2.28-linux-amd64/Documentation/demo.md
etcd-v3.2.28-linux-amd64/Documentation/dev-internal/
etcd-v3.2.28-linux-amd64/Documentation/dev-internal/logging.md
etcd-v3.2.28-linux-amd64/Documentation/dev-internal/discovery_protocol.md
etcd-v3.2.28-linux-amd64/Documentation/dev-internal/_index.md
etcd-v3.2.28-linux-amd64/Documentation/dev-internal/release.md
etcd-v3.2.28-linux-amd64/Documentation/learning/
etcd-v3.2.28-linux-amd64/Documentation/learning/api_guarantees.md
etcd-v3.2.28-linux-amd64/Documentation/learning/glossary.md
etcd-v3.2.28-linux-amd64/Documentation/learning/why.md
etcd-v3.2.28-linux-amd64/Documentation/learning/auth_design.md
etcd-v3.2.28-linux-amd64/Documentation/learning/data_model.md
etcd-v3.2.28-linux-amd64/Documentation/learning/api.md
etcd-v3.2.28-linux-amd64/Documentation/docs.md
etcd-v3.2.28-linux-amd64/Documentation/integrations.md
etcd-v3.2.28-linux-amd64/Documentation/rfc/
etcd-v3.2.28-linux-amd64/Documentation/rfc/_index.md
etcd-v3.2.28-linux-amd64/Documentation/rfc/v3api.md
etcd-v3.2.28-linux-amd64/Documentation/metrics.md
etcd-v3.2.28-linux-amd64/READMEv2-etcdctl.md

{% endhighlight %}

Navigate to the extraced directory

{% highlight console %}

vikki@kubernetes1:~/backup$ ls
etcd etcd-v3.2.28-linux-amd64 pki

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~/backup$ cd etcd-v3.2.28-linux-amd64/

{% endhighlight %}
##### Step 3: Backup the etcd database

Now you will see a etcdctl binary inside the directy.

Use the key, certificate and CA certificate to take backup of the existing etcd database as shown below

{% highlight console %}

vikki@kubernetes1:~/backup/etcd-v3.2.28-linux-amd64$ sudo ETCDCTL_API=3 ./etcdctl --endpoints https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt snapshot save ../etc_database_backup.db
Snapshot saved at ../etc_database_backup.db

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~/backup/etcd-v3.2.28-linux-amd64$ cd ..
vikki@kubernetes1:~/backup$ ls
etcd etc_database_backup.db etcd-v3.2.28-linux-amd64 pki

{% endhighlight %}

> Now we can see the backup has been taken and saved as etc\_database\_backup.db]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[Kubernetes cluster state is saved in etcd datastore. In the post we are going to see how to take a backup for etcd database in kubernetes cluster.]]></summary></entry><entry><title type="html">RBAC in kubernetes</title><link href="http://0.0.0.0:4000/rbac-in-kubernetes/" rel="alternate" type="text/html" title="RBAC in kubernetes" /><published>2019-11-28T16:26:19+00:00</published><updated>2019-11-28T16:26:19+00:00</updated><id>http://0.0.0.0:4000/rbac-in-kubernetes</id><content type="html" xml:base="http://0.0.0.0:4000/rbac-in-kubernetes/"><![CDATA[There are 3 elements involved in RBAC. In this post we are going to see how to provide user level access to resources.

- Subjects - Users or Process that wants access to Kubernetes API
- Resources - Kubernetes API objects like pods, deployments etc
- Verbs - Set of operations like get, watch create etc

### **Setup**

I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.

<!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-6.jpg" class="kg-image"></figure><!--kg-card-end: image-->
##### Step 1: Create a private key

Create a new directly and navigate to the directory

{% highlight console %}

vikki@kubernetes1:~$ mkdir ssl
vikki@kubernetes1:~$ cd ssl/

{% endhighlight %}

Use openssl command a generate a private key _user1.key_

{% highlight console %}

vikki@kubernetes1:~/ssl$ openssl genrsa -out user1.key 2048
Generating RSA private key, 2048 bit long modulus
.......................................................+++
.......................................................................................................................................+++
e is 65537 (0x10001)
vikki@kubernetes1:~/ssl$ ls
user1.key

{% endhighlight %}
##### Step 2: Generate a CSR 

Use the private key generated in the privous step and generate the certificate signing request(csr)

{% highlight console %}

vikki@kubernetes1:~/ssl$ openssl req -new -key user1.key -out user1.csr -subj "/CN=user1/O=vikki.in"

vikki@kubernetes1:~/ssl$ ls
user1.csr user1.key

{% endhighlight %}
##### Step 3: Sign the CSR and generate certificate

The kubernetes cluster have the CA(certificate authority) key and certificate available under /etc/kubernetes/pki location

{% highlight console %}

vikki@kubernetes1:~/ssl$ ls /etc/kubernetes/pki/ca.
ca.crt ca.key  

{% endhighlight %}

Use the CA certificate and key to sign the CSR

{% highlight console %}

vikki@kubernetes1:~/ssl$ sudo openssl x509 -req -in user1.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out user1.crt -days 365
[sudo] password for vikki: 
Signature ok
subject=/CN=user1/O=vikki.in
Getting CA Private Key

vikki@kubernetes1:~/ssl$ ls
user1.crt user1.csr user1.key

{% endhighlight %}

> Now we have the private key _user1.key_ and signed certificate _user1.crt_

##### Step 4: Set credential for the user

Now set credential for the user _user1_ with the private key and the signed certificate.

{% highlight console %}

vikki@kubernetes1:~/ssl$ kubectl config set-credentials user1 --client-certificate=user1.crt --client-key=user1.key 
User "user1" set.

{% endhighlight %}
##### Step 5: Set context for the user

Now set the new context with the username , cluster etc.

We can also map the context to specific namespace using the --namespacec option. By default it will take the default namespace

{% highlight console %}

vikki@kubernetes1:~/ssl$ kubectl config set-context user1-context --cluster=kubernetes --user=user1
Context "user1-context" created.

vikki@kubernetes1:~/ssl$ kubectl config get-contexts 
CURRENT NAME CLUSTER AUTHINFO NAMESPACE
* kubernetes-admin@kubernetes kubernetes kubernetes-admin   
            user1-context kubernetes user1              

{% endhighlight %}

> Now we can see there are 2 context created.

##### Step 6: Create a role

Create a role and map the resources and verb required.

{% highlight console %}

vikki@kubernetes1:~/ssl$ kubectl create role myrole --verb=get,create,list --resource=pods
role.rbac.authorization.k8s.io/myrole created

{% endhighlight %}
##### Step 7: Create a rolebinding

Create a rolebinding and map the role and user.

{% highlight console %}

vikki@kubernetes1:~/ssl$ kubectl create rolebinding myrolebinding --role=myrole --user=user1 
rolebinding.rbac.authorization.k8s.io/myrolebinding created

{% endhighlight %}
##### Step 8: Verify role and rolebinding

List the role and rolebinding and verify both are created.

{% highlight console %}

vikki@kubernetes1:~/ssl$ kubectl get role
NAME AGE
myrole 58s
vikki@kubernetes1:~/ssl$ kubectl get rolebindings.rbac.authorization.k8s.io 
NAME AGE
myrolebinding 8s

{% endhighlight %}
##### Step 9: Change the context and verify role based access
{% highlight console %}

vikki@kubernetes1:~/ssl$ kubectl config get-contexts 
CURRENT NAME CLUSTER AUTHINFO NAMESPACE
* kubernetes-admin@kubernetes kubernetes kubernetes-admin   
            user1-context kubernetes user1     

{% endhighlight %}

Switch to newly created contesxt _user1-context_

{% highlight console %}

vikki@kubernetes1:~/ssl$ kubectl config use-context user1-context 
Switched to context "user1-context".

vikki@kubernetes1:~/ssl$ kubectl config get-contexts 
CURRENT NAME CLUSTER AUTHINFO NAMESPACE
            kubernetes-admin@kubernetes kubernetes kubernetes-admin   
* user1-context kubernetes user1           

{% endhighlight %}

> Now we can see the current context is switched to _user1-context._

Try to create a deployement

{% highlight console %}

vikki@kubernetes1:~/ssl$ kubectl run nginx-deployment --image=nginx
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
Error from server (Forbidden): deployments.apps is forbidden: User "user1" cannot create resource "deployments" in API group "apps" in the namespace "default"

{% endhighlight %}

> The deployment creation is failed because the new context has resource mapped only for pod.

Now lets create a pod and verify the status

{% highlight console %}

vikki@kubernetes1:~/ssl$ vim pod.yaml

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/aa1503f5161a453f120ab6b121f6325a.js"></script><!--kg-card-end: html-->{% highlight console %}

vikki@kubernetes1:~/ssl$ kubectl create -f pod.yaml 
pod/myapp-pod created

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~/ssl$ kubectl get pods
NAME READY STATUS RESTARTS AGE
httpd-7765f5994-vc2j5 1/1 Running 1 2d
myapp-pod 0/1 ContainerCreating 0 5s
nginx-7bffc778db-p4ff5 1/1 Running 1 2d

{% endhighlight %}

> We can see now the pod created successfully and running in the new context.

We can also test the permission of user using the below command

{% highlight console %}

vikki@kubernetes1:~$ kubectl auth can-i create deployments --as user1
no

vikki@kubernetes1:~$ kubectl auth can-i create pods --as user1
yes

{% endhighlight %}]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[There are 3 elements involved in RBAC. In this post we are going to see how to provide user level access to resources.]]></summary></entry><entry><title type="html">Upgrading kubernetes cluster master and worker nodes</title><link href="http://0.0.0.0:4000/upgrading-kubernetes-cluster-master-and-worker-nodes/" rel="alternate" type="text/html" title="Upgrading kubernetes cluster master and worker nodes" /><published>2019-11-26T15:07:58+00:00</published><updated>2019-11-26T15:07:58+00:00</updated><id>http://0.0.0.0:4000/upgrading-kubernetes-cluster-master-and-worker-nodes</id><content type="html" xml:base="http://0.0.0.0:4000/upgrading-kubernetes-cluster-master-and-worker-nodes/"><![CDATA[This post we are going to discuss how to upgrade the kubernetes cluster, both master and worker nodes. We are going to upgrade a older version v1.15 to v.1.16.

### **Setup**

I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.

<!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-5.jpg" class="kg-image"></figure><!--kg-card-end: image-->
### Master node

##### Step 1: Verify the current version of kubelet and kubeadm running in all nodes
{% highlight console %}

vikki@kubernetes1:~$ kubectl get nodes
NAME STATUS ROLES AGE VERSION
kubernetes1 Ready master 41d v1.15.5
kubernetes2 Ready <none> 41d v1.15.5

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.5", GitCommit:"20c265fef0741dd71a66480e35bd69f18351daea", GitTreeState:"clean", BuildDate:"2019-10-15T19:14:19Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"linux/amd64"}

{% endhighlight %}

> We can see kubelet and kubeadm is running in version v1.15.5

##### Step 2: Verify the lastest stable availble version

Run the update in kubernetes master node

{% highlight console %}

root@kubernetes1:~# apt update

{% endhighlight %}{% highlight console %}

root@kubernetes1:~# apt-cache policy kubeadm
kubeadm:
    Installed: 1.15.5-00
    Candidate: 1.16.3-00
    Version table:
    .....

{% endhighlight %}

> We can see from this output that current version is 1.15.5 and the latest available is 1.16.3

##### Step 3: Upgrade the kubeadm to latest version

Upgrade the kubeadm in master to the latest version 1.16.3

{% highlight console %}

root@kubernetes1:~# apt-mark unhold kubeadm && \
> apt-get update && apt-get install -y kubeadm=1.16.3-00 && \
> apt-mark hold kubeadm
kubeadm was already not hold.
Hit:1 http://us.archive.ubuntu.com/ubuntu xenial InRelease                  
Hit:2 http://security.ubuntu.com/ubuntu xenial-security InRelease
Hit:3 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease
Hit:4 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease
Hit:5 https://apt.kubernetes.io kubernetes-xenial InRelease
Reading package lists... Done
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
    kubeadm
1 upgraded, 0 newly installed, 0 to remove and 142 not upgraded.
Need to get 8,762 kB of archives.
After this operation, 4,062 kB of additional disk space will be used.
Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubeadm amd64 1.16.3-00 [8,762 kB]
Fetched 8,762 kB in 7s (1,117 kB/s)                                                                                                                          
(Reading database ... 60808 files and directories currently installed.)
Preparing to unpack .../kubeadm_1.16.3-00_amd64.deb ...
Unpacking kubeadm (1.16.3-00) over (1.15.5-00) ...
Setting up kubeadm (1.16.3-00) ...
kubeadm set on hold.

{% endhighlight %}
##### Step 4: Run the upgrade plan

Now in the previous step we already updated the kubeadm to latest version. Now lets run the upgrade plan and see the available options.

{% highlight console %}

root@kubernetes1:~# sudo kubeadm upgrade plan
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.15.5
[upgrade/versions] kubeadm version: v1.16.3
[upgrade/versions] Latest stable version: v1.16.3
[upgrade/versions] Latest version in the v1.15 series: v1.15.6

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT CURRENT AVAILABLE
Kubelet 2 x v1.15.5 v1.15.6

Upgrade to the latest version in the v1.15 series:

COMPONENT CURRENT AVAILABLE
API Server v1.15.5 v1.15.6
Controller Manager v1.15.5 v1.15.6
Scheduler v1.15.5 v1.15.6
Kube Proxy v1.15.5 v1.15.6
CoreDNS 1.3.1 1.6.2
Etcd 3.3.10 3.3.10

You can now apply the upgrade by executing the following command:

    kubeadm upgrade apply v1.15.6

_____________________________________________________________________

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT CURRENT AVAILABLE
Kubelet 2 x v1.15.5 v1.16.3

Upgrade to the latest stable version:

COMPONENT CURRENT AVAILABLE
API Server v1.15.5 v1.16.3
Controller Manager v1.15.5 v1.16.3
Scheduler v1.15.5 v1.16.3
Kube Proxy v1.15.5 v1.16.3
CoreDNS 1.3.1 1.6.2
Etcd 3.3.10 3.3.15-0

You can now apply the upgrade by executing the following command:

    kubeadm upgrade apply v1.16.3

_____________________________________________________________________

{% endhighlight %}

> We can see that we can upgrade the cluster to v1.16.3 from the above output

##### Step 5: Drain the pods in master node

Now drain all the pods execpt daemonsets in the master node

{% highlight console %}

vikki@kubernetes1:~$ kubectl drain kubernetes1 --ignore-daemonsets
node/kubernetes1 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-bfr9j, kube-system/kube-proxy-8267n
evicting pod "coredns-5c98db65d4-v4cms"
evicting pod "coredns-5c98db65d4-g4gv4"
pod/coredns-5c98db65d4-v4cms evicted
pod/coredns-5c98db65d4-g4gv4 evicted
node/kubernetes1 evicted

{% endhighlight %}
##### Step 6: Run the upgrade

Now run the upgrade to the lastest version 1.16.3

{% highlight console %}

vikki@kubernetes1:~$ sudo kubeadm upgrade apply v1.16.3
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/version] You have chosen to change the cluster version to "v1.16.3"
[upgrade/versions] Cluster version: v1.15.5
[upgrade/versions] kubeadm version: v1.16.3
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd]
[upgrade/prepull] Prepulling image for component etcd.
[upgrade/prepull] Prepulling image for component kube-apiserver.
[upgrade/prepull] Prepulling image for component kube-controller-manager.
[upgrade/prepull] Prepulling image for component kube-scheduler.
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler
[upgrade/prepull] Prepulled image for component etcd.
[upgrade/prepull] Prepulled image for component kube-apiserver.
[upgrade/prepull] Prepulled image for component kube-controller-manager.
[upgrade/prepull] Prepulled image for component kube-scheduler.
[upgrade/prepull] Successfully prepulled the images for all the control plane components
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.16.3"...
Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e
Static pod: kube-controller-manager-kubernetes1 hash: f106f0d94b93b77e4db974b1c477d277
Static pod: kube-scheduler-kubernetes1 hash: 131c3f63daec7c0750818f64a2f75d20
[upgrade/etcd] Upgrading to TLS for etcd
Static pod: etcd-kubernetes1 hash: 352a2620657e49493504dc8f27c83195
[upgrade/staticpods] Preparing for "etcd" upgrade
[upgrade/staticpods] Renewing etcd-server certificate
[upgrade/staticpods] Renewing etcd-peer certificate
[upgrade/staticpods] Renewing etcd-healthcheck-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/etcd.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/etcd.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: etcd-kubernetes1 hash: 352a2620657e49493504dc8f27c83195
Static pod: etcd-kubernetes1 hash: 3ce412f7cfe0c06939809c93f738e798
[apiclient] Found 1 Pods for label selector component=etcd
[upgrade/staticpods] Component "etcd" upgraded successfully!
[upgrade/etcd] Waiting for etcd to become available
[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests001214141"
[upgrade/staticpods] Preparing for "kube-apiserver" upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/kube-apiserver.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e
Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e
Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e
Static pod: kube-apiserver-kubernetes1 hash: d8708cb2f25dd11bbb41dd9729149325
[apiclient] Found 1 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component "kube-apiserver" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-controller-manager" upgrade
[upgrade/staticpods] Renewing controller-manager.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/kube-controller-manager.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-controller-manager-kubernetes1 hash: f106f0d94b93b77e4db974b1c477d277
Static pod: kube-controller-manager-kubernetes1 hash: e6e76bb8264f2e84070efada35e93e71
[apiclient] Found 1 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-scheduler" upgrade
[upgrade/staticpods] Renewing scheduler.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/kube-scheduler.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
Static pod: kube-scheduler-kubernetes1 hash: 131c3f63daec7c0750818f64a2f75d20
Static pod: kube-scheduler-kubernetes1 hash: 8c5e33e50bb56e8adacd1cc99c56b2cb
[apiclient] Found 1 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component "kube-scheduler" upgraded successfully!
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.16" in namespace kube-system with the configuration for the kubelets in the cluster
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.16" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.16.3". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.

{% endhighlight %}

> We can see the cluster is upgraded to lastest version v1.16.3

##### Step 7: Uncordon the master node

Now ucordon the master node

{% highlight console %}

vikki@kubernetes1:~$ kubectl uncordon kubernetes1 
node/kubernetes1 uncordoned

{% endhighlight %}
##### Step 8: Upgrade the kubelet 

Now in the previous steps we have upgraded the kubeadm and cluster , next we need to upgrade the kubelet and kubectl

{% highlight console %}

root@kubernetes1:~# apt-mark unhold kubelet kubectl && \
> apt-get update && apt-get install -y kubelet=1.16.3-00 kubectl=1.16.3-00 && \
> apt-mark hold kubelet kubectl
kubelet was already not hold.
kubectl was already not hold.
Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease           
Hit:2 http://us.archive.ubuntu.com/ubuntu xenial InRelease
Hit:3 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease
Hit:4 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease
Hit:5 https://apt.kubernetes.io kubernetes-xenial InRelease
Reading package lists... Done
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
    kubectl kubelet
2 upgraded, 0 newly installed, 0 to remove and 140 not upgraded.
Need to get 29.9 MB of archives.
After this operation, 7,134 kB of additional disk space will be used.
Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubectl amd64 1.16.3-00 [9,233 kB]
Get:2 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubelet amd64 1.16.3-00 [20.7 MB]                                                               
Fetched 29.9 MB in 8s (3,436 kB/s)                                                                                                                           
(Reading database ... 60808 files and directories currently installed.)
Preparing to unpack .../kubectl_1.16.3-00_amd64.deb ...
Unpacking kubectl (1.16.3-00) over (1.15.5-00) ...
Preparing to unpack .../kubelet_1.16.3-00_amd64.deb ...
Unpacking kubelet (1.16.3-00) over (1.15.5-00) ...
Setting up kubectl (1.16.3-00) ...
Setting up kubelet (1.16.3-00) ...
kubelet set on hold.
kubectl set on hold.

{% endhighlight %}

Restart the kubelet service

{% highlight console %}

root@kubernetes1:~# sudo systemctl restart kubelet

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get nodes
NAME STATUS ROLES AGE VERSION
kubernetes1 Ready master 41d v1.16.3
kubernetes2 Ready <none> 41d v1.15.5

{% endhighlight %}

> Now we can see the master node kubernetes1 is upgraded to v1.16.3

### Worker node

##### Step 1: Upgrade the kubeadm to version 1.16.3
{% highlight console %}

root@kubernetes2:~# apt-mark unhold kubeadm && \
> apt-get update && apt-get install -y kubeadm=1.16.3-00 && \
> apt-mark hold kubeadm
kubeadm was already not hold.
Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [109 kB]                 
Get:2 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [785 kB]       
Hit:3 http://us.archive.ubuntu.com/ubuntu xenial InRelease
Get:4 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]      
Get:5 https://apt.kubernetes.io kubernetes-xenial InRelease [161 B]
Get:6 https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages [31.3 kB]                      
Get:7 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]        
Get:8 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [1,071 kB]             
Get:9 http://security.ubuntu.com/ubuntu xenial-security/main i386 Packages [617 kB]     
Get:10 http://security.ubuntu.com/ubuntu xenial-security/main Translation-en [302 kB]                                                                        
Get:11 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [466 kB]                                                                    
Get:12 http://us.archive.ubuntu.com/ubuntu xenial-updates/main i386 Packages [882 kB]                                                                        
Get:13 http://security.ubuntu.com/ubuntu xenial-security/universe i386 Packages [401 kB]                                                                     
Get:14 http://security.ubuntu.com/ubuntu xenial-security/universe Translation-en [191 kB]                                                                    
Get:15 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [5,724 B]                                                                 
Get:16 http://security.ubuntu.com/ubuntu xenial-security/multiverse i386 Packages [5,888 B]                                                                  
Get:17 http://us.archive.ubuntu.com/ubuntu xenial-updates/main Translation-en [413 kB]                                                                       
Get:18 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [771 kB]                                                                   
Get:19 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe i386 Packages [700 kB]                                                                    
Get:20 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe Translation-en [324 kB]                                                                   
Get:21 http://us.archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [16.8 kB]                                                                
Get:22 http://us.archive.ubuntu.com/ubuntu xenial-updates/multiverse i386 Packages [15.9 kB]                                                                 
Fetched 7,333 kB in 23s (315 kB/s)                                                                                                                           
Reading package lists... Done
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
    kubeadm
1 upgraded, 0 newly installed, 0 to remove and 139 not upgraded.
Need to get 8,762 kB of archives.
After this operation, 4,062 kB of additional disk space will be used.
Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubeadm amd64 1.16.3-00 [8,762 kB]
Fetched 8,762 kB in 7s (1,122 kB/s)                                                                                                                          
(Reading database ... 60808 files and directories currently installed.)
Preparing to unpack .../kubeadm_1.16.3-00_amd64.deb ...
Unpacking kubeadm (1.16.3-00) over (1.15.5-00) ...
Setting up kubeadm (1.16.3-00) ...
kubeadm set on hold.

{% endhighlight %}
##### Step 2: Drain the pods in worker node

Now drain all the pods , except daemonsets from the worker node kubernetes2. _This command should be run in master node in case the kubenetes config is not mapped in worker_.

{% highlight console %}

vikki@kubernetes1:~$ kubectl drain kubernetes2 --ignore-daemonsets
node/kubernetes2 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-8v6lb, kube-system/kube-proxy-877b2
evicting pod "coredns-5644d7b6d9-t8qw5"
evicting pod "httpd-7765f5994-gvlj6"
evicting pod "nginx-7bffc778db-phdb5"
evicting pod "coredns-5644d7b6d9-frz4v"
pod/coredns-5644d7b6d9-frz4v evicted
pod/coredns-5644d7b6d9-t8qw5 evicted
pod/httpd-7765f5994-gvlj6 evicted
pod/nginx-7bffc778db-phdb5 evicted
node/kubernetes2 evicted

{% endhighlight %}
##### Step 3: Upgrade the worker node
{% highlight console %}

root@kubernetes2:~# sudo kubeadm upgrade node
[upgrade] Reading configuration from the cluster...
[upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[upgrade] Skipping phase. Not a control plane node[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.16" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[upgrade] The configuration for this node was successfully updated!
[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.

{% endhighlight %}
##### Step 4: Upgrade the kubelet and kubectl

In previous steps we upgraded the kubeadm and cluster. Now we need to upgrade the kubelet and kubectl.

{% highlight console %}

root@kubernetes2:~# apt-mark unhold kubelet kubectl && \
> apt-get update && apt-get install -y kubelet=1.16.3-00 kubectl=1.16.3-00 && \
> apt-mark hold kubelet kubectl
kubelet was already not hold.
kubectl was already not hold.
Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Hit:2 http://us.archive.ubuntu.com/ubuntu xenial InRelease                       
Hit:3 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease
Hit:4 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease
Hit:5 https://apt.kubernetes.io kubernetes-xenial InRelease
Reading package lists... Done
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
    kubectl kubelet
2 upgraded, 0 newly installed, 0 to remove and 137 not upgraded.
Need to get 29.9 MB of archives.
After this operation, 3,447 kB of additional disk space will be used.
Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubectl amd64 1.16.3-00 [9,233 kB]
Get:2 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubelet amd64 1.16.3-00 [20.7 MB]                                                               
Fetched 29.9 MB in 32s (934 kB/s)                                                                                                                            
(Reading database ... 60808 files and directories currently installed.)
Preparing to unpack .../kubectl_1.16.3-00_amd64.deb ...
Unpacking kubectl (1.16.3-00) over (1.16.2-00) ...
Preparing to unpack .../kubelet_1.16.3-00_amd64.deb ...
Unpacking kubelet (1.16.3-00) over (1.15.5-00) ...
Setting up kubectl (1.16.3-00) ...
Setting up kubelet (1.16.3-00) ...
kubelet set on hold.
kubectl set on hold.

{% endhighlight %}

Restart the kubelet service in worker node

{% highlight console %}

root@kubernetes2:~# sudo systemctl restart kubelet

{% endhighlight %}
##### Step 5: Uncordon the worker node

Now uncordon all the pods in kubernetes2 worker onde

{% highlight console %}

vikki@kubernetes1:~$ kubectl uncordon kubernetes2
node/kubernetes2 uncordoned

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get nodes
NAME STATUS ROLES AGE VERSION
kubernetes1 Ready master 41d v1.16.3
kubernetes2 Ready <none> 41d v1.16.3

{% endhighlight %}

> We can see now both the master and worker nodes are upgraded from v1.15.5 to v1.16.3]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[This post we are going to discuss how to upgrade the kubernetes cluster, both master and worker nodes. We are going to upgrade a older version v1.15 to v.1.16.]]></summary></entry><entry><title type="html">Running kubernetes custom scheduler</title><link href="http://0.0.0.0:4000/running-kubernetes-custom-scheduler/" rel="alternate" type="text/html" title="Running kubernetes custom scheduler" /><published>2019-11-25T17:28:55+00:00</published><updated>2019-11-25T17:28:55+00:00</updated><id>http://0.0.0.0:4000/running-kubernetes-custom-scheduler</id><content type="html" xml:base="http://0.0.0.0:4000/running-kubernetes-custom-scheduler/"><![CDATA[Kubernetes cluster have a default scheduler kube-scheduler. If the default scheduler does not suits our requirement we can also create our own scheduler. In the post we will discus how to create multiple scheduler and schedule pods based on different scheduler.

### **Setup**

I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.

<!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-4.jpg" class="kg-image"></figure><!--kg-card-end: image-->
##### Step 1: Copy the default scheduler yaml and modify

Copy the default scheduler yaml from master node and modify the name.

In this below example i am naming the custom schedule are _my-scheduler._ Add a ServiceAccount and ClusterRoleBinding to the yaml file as given below.

The key changes made are

- line 27: name: my-scheduler
- line 30: serviceAccountName: my-scheduler
- line 38: - --leader-elect=false
- line 39: - --scheduler-name=my-scheduler
{% highlight console %}

vikki@kubernetes1:~$ sudo cp /etc/kubernetes/manifests/kube-scheduler.yaml custom-scheduler.yaml
vikki@kubernetes1:~$ sudo chmod 777 custom-scheduler.yaml 
vikki@kubernetes1:~$ vim custom-scheduler.yaml 

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/2a8e765cb702a7f8edf4e8760599da10.js"></script><!--kg-card-end: html-->
##### Step 2: Create the custom scheduler
{% highlight console %}

vikki@kubernetes1:~$ kubectl create -f custom-scheduler.yaml 
serviceaccount/my-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/my-scheduler-as-kube-scheduler created
pod/my-scheduler created

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get pods -n=kube-system 
NAME READY STATUS RESTARTS AGE
calico-kube-controllers-55754f75c-fgs8g 1/1 Running 7 22d
calico-node-4h72l 1/1 Running 7 22d
calico-node-ld84s 1/1 Running 7 22d
calico-node-lrfz9 1/1 Running 2 31h
calico-node-ws576 1/1 Running 1 27h
coredns-5644d7b6d9-2g6rs 1/1 Running 7 22d
coredns-5644d7b6d9-ccxsg 1/1 Running 7 22d
etcd-kubernetes1 1/1 Running 8 22d
kube-apiserver-kubernetes1 1/1 Running 8 22d
kube-controller-manager-kubernetes1 1/1 Running 8 22d
kube-proxy-6xd8l 1/1 Running 2 31h
kube-proxy-96q5x 1/1 Running 7 22d
kube-proxy-njl6r 1/1 Running 7 22d
kube-proxy-whlhw 1/1 Running 1 27h
kube-scheduler-kubernetes1 1/1 Running 8 22d
my-scheduler 1/1 Running 0 6s

{% endhighlight %}

> Now we can see a new custom scheduler _my-scheduler_ is running along with defautl scheduler _kube-scheduler-kubernetes1_

##### Step 3: Edit the custerrole for kube-scheduler

If RBAC is enabled on your cluster, you must update the system:kube-scheduler cluster role. Edit the clusterrole for kube-scheduler and modify.

Below are the key changes made in orignal file

- line 33: - my-scheduler
- line 131: - storageclasses
{% highlight console %}

vikki@kubernetes1:~$ kubectl edit clusterrole system:kube-scheduler

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/801aa697779378ff30e46e5247de8980.js"></script><!--kg-card-end: html-->
##### Step 4: Create pods with different types of scheduler

Create a pod without explicitly mentioning any scheuler name. This pod should be scheduled by default scheduler _kube-scheduler_

{% highlight console %}

vikki@kubernetes1:~$ vim pod_default_scheduler.yaml

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/cfe05dda5b00a4f170b9cdd08f6aa0dd.js"></script><!--kg-card-end: html-->

Create a pod by explicitly mentioning custom scheuler name _my-scheduler_.

{% highlight console %}

vikki@kubernetes1:~$ vim pod_custom_scheduler.yaml 

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/04cc5ac3e1933b4c1979a631de424116.js"></script><!--kg-card-end: html-->{% highlight console %}

vikki@kubernetes1:~$ kubectl create -f pod_default_scheduler.yaml 
pod/nginx-pod-default-scheduler created
vikki@kubernetes1:~$ kubectl create -f pod_custom_scheduler.yaml 
pod/nginx-pod-custom-scheduler created

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get pods nginx-pod-custom-scheduler nginx-pod-default-scheduler
NAME READY STATUS RESTARTS AGE
nginx-pod-custom-scheduler 1/1 Running 0 9m9s
nginx-pod-default-scheduler 1/1 Running 0 9m13s
vikki@kubernetes1:~$ 

{% endhighlight %}

> Now we can see both the pods are created and running

{% highlight console %}

vikki@kubernetes1:~$ kubectl get events -o wide |grep nginx-pod-custom-scheduler |head -1
<unknown> Normal Scheduled pod/nginx-pod-custom-scheduler my-scheduler Successfully assigned default/nginx-pod-custom-scheduler to kubernetes3 <unknown> 0 nginx-pod-custom-scheduler.15da76383c2c2859

vikki@kubernetes1:~$ kubectl get events -o wide |grep nginx-pod-default-scheduler |head -1
<unknown> Normal Scheduled pod/nginx-pod-default-scheduler default-scheduler Successfully assigned default/nginx-pod-default-scheduler to kubernetes3 <unknown> 0 nginx-pod-default-scheduler.15da76372f4444f7

{% endhighlight %}

> From the events, we can see pod nginx-pod-custom-scheduler is scheduled by _my-scheduler_ and nginx-pod-default-scheduler by &nbsp;_default-scheduler._]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[Kubernetes cluster have a default scheduler kube-scheduler. If the default scheduler does not suits our requirement we can also create our own scheduler. In the post we will discus how to create multiple scheduler and schedule pods based on different scheduler.]]></summary></entry><entry><title type="html">Creating static pod in kubernetes</title><link href="http://0.0.0.0:4000/creating-static-pod-in-kubernetes/" rel="alternate" type="text/html" title="Creating static pod in kubernetes" /><published>2019-11-24T15:55:52+00:00</published><updated>2019-11-24T15:55:52+00:00</updated><id>http://0.0.0.0:4000/creating-static-pod-in-kubernetes</id><content type="html" xml:base="http://0.0.0.0:4000/creating-static-pod-in-kubernetes/"><![CDATA[Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them.  
Static pods automatically restarts if it crashes. Static Pods are always bound to one Kubelet on a specific node. In the post we will try creating a static pod and watch the behaviour on delete.

### **Setup**

I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.

<!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-3.jpg" class="kg-image"></figure><!--kg-card-end: image-->
##### Step 1: Find the static pod manifest location

Go to any node where you want to run the static pod. I want to run in kubernetes3 node. Look for the kubelet process and config.yaml file associated with it

{% highlight console %}

root@kubernetes3:~# ps -aux |grep kubelet
root 905 3.6 9.4 544516 95676 ? Ssl 16:49 9:13 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=cgroupfs --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1
root 29041 0.0 0.0 14224 940 pts/0 S+ 21:03 0:00 grep --color=auto kubelet

{% endhighlight %}

Now grep for the staticPodPath in config file

{% highlight console %}

root@kubernetes3:~# cat /var/lib/kubelet/config.yaml |grep static
staticPodPath: /etc/kubernetes/manifests

{% endhighlight %}
##### Step 2: Go to the directory and add a yaml for pod
{% highlight console %}

root@kubernetes3:~# cd /etc/kubernetes/manifests/
root@kubernetes3:/etc/kubernetes/manifests# vim static-pod.yaml

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/bbd1584780b98d771c479a4413c97b6e.js"></script><!--kg-card-end: html-->

Now try to grep for the pod name in the docker container list

{% highlight console %}

root@kubernetes3:/etc/kubernetes/manifests# docker ps -a |grep static-web
e5bf8054343a nginx "nginx -g 'daemon of…" 17 seconds ago Up 16 seconds k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0
138ea3819894 k8s.gcr.io/pause:3.1 "/pause" 23 seconds ago Up 22 seconds k8s_POD_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0

{% endhighlight %}

> We can see a new static pod is created automatically

##### Step 3: Delete the container and watch the behaviour

Now lets try to delete the static pod &nbsp;

{% highlight console %}

root@kubernetes3:/etc/kubernetes/manifests# docker stop k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0
k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0
root@kubernetes3:/etc/kubernetes/manifests# docker rm k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0
k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0

{% endhighlight %}{% highlight console %}

root@kubernetes3:/etc/kubernetes/manifests# docker ps -a |grep static-web
d3d9713b937e nginx "nginx -g 'daemon of…" 1 second ago Up Less than a second k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_1
138ea3819894 k8s.gcr.io/pause:3.1 "/pause" About a minute ago Up About a minute k8s_POD_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0

{% endhighlight %}

> We can see the new static pod is automatically creating]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. Static pods automatically restarts if it crashes. Static Pods are always bound to one Kubelet on a specific node. In the post we will try creating a static pod and watch the behaviour on delete.]]></summary></entry><entry><title type="html">Pod scheduling in kubernetes - detailed step by step</title><link href="http://0.0.0.0:4000/pod-scheduling-in-kubernetes-detailed-step-by-step/" rel="alternate" type="text/html" title="Pod scheduling in kubernetes - detailed step by step" /><published>2019-11-24T14:42:51+00:00</published><updated>2019-11-24T14:42:51+00:00</updated><id>http://0.0.0.0:4000/pod-scheduling-in-kubernetes-detailed-step-by-step</id><content type="html" xml:base="http://0.0.0.0:4000/pod-scheduling-in-kubernetes-detailed-step-by-step/"><![CDATA[We can assign the pod to node based on various methods. Lets discuss all the below methods in the post

- Using nodeName
- Using labels in nodeSelector
- Node Affinity/Anti Affinity
- Pod Affinity/Anti Affinity
- Taints and tolerations

### **Setup**

I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 and 512GB SSD hardisk.

<!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="/content/images/2019/11/setup-2.jpg" class="kg-image"></figure><!--kg-card-end: image-->
### Using nodeName

##### Step 1: Create a pod and assign using nodeName
{% highlight console %}

vikki@kubernetes1:~$ vim pod_node_name.yaml

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/5f329160b5846ba72beba94bd46f8860.js"></script><!--kg-card-end: html-->{% highlight console %}

vikki@kubernetes1:~$ kubectl apply -f pod_node_name.yaml 
pod/nginx-pod-nodename created

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get pods -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
busybox 1/1 Running 4 5h47m 192.168.249.141 kubernetes2 <none> <none>
nginx-pod-nodename 1/1 Running 0 8s 192.168.80.199 kubernetes3 <none> <none>
web-0 1/1 Running 2 5h21m 192.168.249.139 kubernetes2 <none> <none>
web-1 1/1 Running 2 5h21m 192.168.249.140 kubernetes2 <none> <none>
web-2 1/1 Running 2 5h22m 192.168.249.138 kubernetes2 <none> <none>

{% endhighlight %}

> Now we can see the pod is created in the node "kubernetes3" conifgued in nodeName option

<!--kg-card-begin: hr-->
* * *
<!--kg-card-end: hr-->
### Using labels in nodeSelector

##### Step 1: Add a new label to the node

Check the current lables using the below command

{% highlight console %}

vikki@kubernetes1:~$ kubectl get nodes --show-labels 
NAME STATUS ROLES AGE VERSION LABELS
kubernetes1 Ready master 20d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes1,kubernetes.io/os=linux,node-role.kubernetes.io/master=
kubernetes2 Ready <none> 20d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes2,kubernetes.io/os=linux
kubernetes3 Ready <none> 117m v1.16.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes3,kubernetes.io/os=linux,namee=node3

{% endhighlight %}

Add a new lable "disktype=vhd" to the kubernetes3 node

{% highlight console %}

vikki@kubernetes1:~$ kubectl label nodes kubernetes3 disktype=vhd
node/kubernetes3 labeled

vikki@kubernetes1:~$ kubectl get nodes --show-labels 
NAME STATUS ROLES AGE VERSION LABELS
kubernetes1 Ready master 20d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes1,kubernetes.io/os=linux,node-role.kubernetes.io/master=
kubernetes2 Ready <none> 20d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes2,kubernetes.io/os=linux
kubernetes3 Ready <none> 118m v1.16.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=vhd,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes3,kubernetes.io/os=linux,namee=node3

{% endhighlight %}
##### Step 2: Create a pod and assign using nodeSelector
{% highlight console %}

vikki@kubernetes1:~$ vim pod_label.yaml

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/dc26cbe554bd78b4e17103bcd96d153f.js"></script><!--kg-card-end: html-->{% highlight console %}

vikki@kubernetes1:~$ kubectl create -f pod_label.yaml 
pod/nginx-pod-label created

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get pods -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
busybox 1/1 Running 3 4h50m 192.168.249.141 kubernetes2 <none> <none>
nginx-pod-label 1/1 Running 0 4m14s 192.168.80.195 kubernetes3 <none> <none>
web-0 1/1 Running 2 4h25m 192.168.249.139 kubernetes2 <none> <none>
web-1 1/1 Running 2 4h25m 192.168.249.140 kubernetes2 <none> <none>
web-2 1/1 Running 2 4h25m 192.168.249.138 kubernetes2 <none> <none>
vikki@kubernetes1:~$ 

{% endhighlight %}

> Now we can see the new pod is assinged to kubernetes3 based on nodeSelector label option

<!--kg-card-begin: hr-->
* * *
<!--kg-card-end: hr-->
### Advance pod scheduling

We can also assing the pod to a specific node using the Node/pod affinity and anti affinity rules.

Affinity types:

- requiredDuringSchedulingRequiredDuringExecution
- requiredDuringSchedulingIgnoredDuringExecution
- preferredDuringSchedulingIgnoredDuringExecution

Affinity operators:

- In
- NotIn
- Exists
- DoesNotExist
- Gt
- Lt

### Node Affinity/Anti Affinity

##### Step 1: Create a pod with node affinity 

Create a pod with node affinity specs and match the lable using matchExpressions spec

{% highlight console %}

vikki@kubernetes1:~$ vim pod_node_affinity.yaml 

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/704e4a3ce125a5e66b5b754f9edcd874.js"></script><!--kg-card-end: html-->{% highlight console %}

vikki@kubernetes1:~$ kubectl create -f pod_node_affinity.yaml 
pod/nginx-pod-nodeaffinity created

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get pods
NAME READY STATUS RESTARTS AGE
busybox 1/1 Running 3 5h17m
nginx-pod-label 1/1 Running 0 31m
nginx-pod-nodeaffinity 0/1 Pending 0 4s
web-0 1/1 Running 2 4h52m
web-1 1/1 Running 2 4h52m
web-2 1/1 Running 2 4h52m

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl describe pods nginx-pod-nodeaffinity |grep Events: -A 3
Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
    Warning FailedScheduling <unknown> default-scheduler 0/3 nodes are available: 3 node(s) didn't match node selector.
    

{% endhighlight %}

> Now the pod is failing because there is no node has the match lables

Lets add a label bandwidth to the kubernetes3 node

{% highlight console %}

vikki@kubernetes1:~$ kubectl label nodes kubernetes3 bandwidth=100GB
node/kubernetes3 labeled

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl describe pods nginx-pod-nodeaffinity |grep Events: -A 5
Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
    Warning FailedScheduling <unknown> default-scheduler 0/3 nodes are available: 3 node(s) didn't match node selector.
    Warning FailedScheduling <unknown> default-scheduler 0/3 nodes are available: 3 node(s) didn't match node selector.
    Normal Scheduled <unknown> default-scheduler Successfully assigned default/nginx-pod-nodeaffinity to kubernetes3

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get pods -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
busybox 1/1 Running 3 5h23m 192.168.249.141 kubernetes2 <none> <none>
nginx-pod-label 1/1 Running 0 37m 192.168.80.195 kubernetes3 <none> <none>
nginx-pod-nodeaffinity 1/1 Running 0 6m16s 192.168.80.196 kubernetes3 <none> <none>
web-0 1/1 Running 2 4h58m 192.168.249.139 kubernetes2 <none> <none>
web-1 1/1 Running 2 4h58m 192.168.249.140 kubernetes2 <none> <none>
web-2 1/1 Running 2 4h59m 192.168.249.138 kubernetes2 <none> <none>
    

{% endhighlight %}

> Now we can see the node pod is successully assinged to the kubernetes3 node after adding the label

<!--kg-card-begin: hr-->
* * *
<!--kg-card-end: hr-->
### Pod Affinity/Anti Affinity

We can also assing the pod to a specific node using the pod affinity and anti affinity rules.

##### Step 1: Create a pod with pod affinity

Create a pod with pod affinity specs and match the lable using matchExpressions spec

{% highlight console %}

vikki@kubernetes1:~$ vim pod_pod_affinity.yaml 

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/a4048e59449210e28e4bf66971c8d56a.js"></script><!--kg-card-end: html-->{% highlight console %}

vikki@kubernetes1:~$ kubectl create -f pod_pod_affinity.yaml 
pod/nginx-pod-podaffinity created

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get pods
NAME READY STATUS RESTARTS AGE
busybox 1/1 Running 4 5h36m
nginx-pod-label 1/1 Running 0 49m
nginx-pod-nodeaffinity 1/1 Running 0 18m
nginx-pod-podaffinity 0/1 Pending 0 4s
web-0 1/1 Running 2 5h10m
web-1 1/1 Running 2 5h10m
web-2 1/1 Running 2 5h11m

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl describe pods nginx-pod-podaffinity |grep Events: -A 5 
Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
    Warning FailedScheduling <unknown> default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 node(s) didn't match pod affinity rules, 2 node(s) didn't match pod affinity/anti-affinity.
    Warning FailedScheduling <unknown> default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 node(s) didn't match pod affinity rules, 2 node(s) didn't match pod affinity/anti-affinity.

{% endhighlight %}

> Now the pod is failing because there is no pod running in any nodes that &nbsp;has the match lables

Lets create a new pod with the label configued previously

{% highlight console %}

vikki@kubernetes1:~$ vim pod_label_podaffinity.yaml

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/09965427e66db5bc14443f49f2ed2a9d.js"></script><!--kg-card-end: html-->{% highlight console %}

vikki@kubernetes1:~$ kubectl apply -f pod_label_podaffinity.yaml 
pod/nginx-pod-web-nginx-backend created

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl describe pods nginx-pod-podaffinity |grep Events: -A 5 
Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
    Warning FailedScheduling <unknown> default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 node(s) didn't match pod affinity rules, 2 node(s) didn't match pod affinity/anti-affinity.
    Warning FailedScheduling <unknown> default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 node(s) didn't match pod affinity rules, 2 node(s) didn't match pod affinity/anti-affinity.
    Normal Scheduled <unknown> default-scheduler Successfully assigned default/nginx-pod-podaffinity to kubernetes3

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get pods -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
busybox 1/1 Running 4 5h39m 192.168.249.141 kubernetes2 <none> <none>
nginx-pod-label 1/1 Running 0 52m 192.168.80.195 kubernetes3 <none> <none>
nginx-pod-nodeaffinity 1/1 Running 0 21m 192.168.80.196 kubernetes3 <none> <none>
nginx-pod-podaffinity 1/1 Running 0 3m2s 192.168.80.197 kubernetes3 <none> <none>
nginx-pod-web-nginx-backend 1/1 Running 0 14s 192.168.80.198 kubernetes3 <none> <none>
web-0 1/1 Running 2 5h13m 192.168.249.139 kubernetes2 <none> <none>
web-1 1/1 Running 2 5h13m 192.168.249.140 kubernetes2 <none> <none>
web-2 1/1 Running 2 5h14m 192.168.249.138 kubernetes2 <none> <none>

{% endhighlight %}

> Now we can see only the pod with lable app: web-nginx-backend is created, the previous pod is also created in the same node

<!--kg-card-begin: hr-->
* * *
<!--kg-card-end: hr-->
### Taints and tolerations

Node affinity is a property of pods that attracts them to a set of nodes. Taints are the opposite, they allow a node to repel a set of pods.

##### Step 1: Create a label to node and assign pod using nodeSelector
{% highlight console %}

vikki@kubernetes1:~$ kubectl label nodes kubernetes4 app=highperformance
node/kubernetes4 labeled
vikki@kubernetes1:~$ kubectl get nodes kubernetes4 --show-labels 
NAME STATUS ROLES AGE VERSION LABELS
kubernetes4 Ready <none> 2m55s v1.16.3 app=highperformance,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes4,kubernetes.io/os=linux

{% endhighlight %}{% highlight console %}

    vikki@kubernetes1:~$ vim pod_label_1.yaml

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/fb98260bd5405feee85ca565b188933b.js"></script><!--kg-card-end: html-->{% highlight console %}

vikki@kubernetes1:~$ kubectl create -f pod_label_1.yaml 
pod/nginx-pod-taint created

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get pod nginx-pod-taint -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
nginx-pod-taint 1/1 Running 0 10m 192.168.48.129 kubernetes4 <none> <none>

{% endhighlight %}

> We can see the new pod nginx-pod-taint is created and assigned to kubernetes4 node

##### Step 2: Create a taint

Now lets create a taint "NoSchedule" and add to the kubernetes4 node.

{% highlight console %}

vikki@kubernetes1:~$ kubectl taint nodes kubernetes4 key1=value1:NoSchedule
node/kubernetes4 tainted

vikki@kubernetes1:~$ kubectl get nodes kubernetes4 -o yaml |grep -i taint -A 3
    taints:
    - effect: NoSchedule
    key: key1
    value: value1

{% endhighlight %}
##### Step 3: Create a new pod and try assign to kubernetes4

Now create a new pod and use nodeSelector to assign to kubernetes4

{% highlight console %}

vikki@kubernetes1:~$ vim pod_label_2.yaml

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/4da565b636ec481e4f6e5147b2419a93.js"></script><!--kg-card-end: html-->{% highlight console %}

vikki@kubernetes1:~$ kubectl create -f pod_label_2.yaml 
pod/nginx-pod-taint-2 created

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get pods nginx-pod-taint-2 -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
nginx-pod-taint-2 0/1 Pending 0 7s <none> <none> <none> <none>

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl describe pod nginx-pod-taint-2 |grep -i events: -A 5
Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
    Warning FailedScheduling <unknown> default-scheduler 0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 node(s) didn't match node selector.
    Warning FailedScheduling <unknown> default-scheduler 0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 node(s) didn't match node selector.

{% endhighlight %}

> We can see the pod creatation if failing due to the taints setting.

##### Step 4: Update the pod with tolerations

Now lets add toleration to the same pod for "NoSchedule" and apply the changes.

{% highlight console %}

vikki@kubernetes1:~$ vim pod_label_3.yaml 

{% endhighlight %}<!--kg-card-begin: html--><script src="https://gist.github.com/vigneshragupathy/5790929db238b9276d4961d3a91e6a29.js"></script><!--kg-card-end: html-->{% highlight console %}

vikki@kubernetes1:~$ kubectl apply -f pod_label_3.yaml 
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
pod/nginx-pod-taint-2 configured

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl describe pod nginx-pod-taint-2 |grep -i events: -A 5
Events:
    Type Reason Age From Message
    ---- ------ ---- ---- -------
    Warning FailedScheduling <unknown> default-scheduler 0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 node(s) didn't match node selector.
    Warning FailedScheduling <unknown> default-scheduler 0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 node(s) didn't match node selector.
    Normal Scheduled <unknown> default-scheduler Successfully assigned default/nginx-pod-taint-2 to kubernetes4

{% endhighlight %}{% highlight console %}

vikki@kubernetes1:~$ kubectl get pod nginx-pod-taint-2 -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
nginx-pod-taint-2 1/1 Running 0 11m 192.168.48.130 kubernetes4 <none> <none>

{% endhighlight %}

> Now we can see the pod changed from failed to success state and assinged to kubernetes4 node according to the nodeSelector.]]></content><author><name>Vignesh Ragupathy</name></author><category term="kubernetes" /><category term="linux" /><category term="opensource" /><summary type="html"><![CDATA[We can assign the pod to node based on various methods. Lets discuss all the below methods in the post]]></summary></entry></feed>